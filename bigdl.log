2019-06-28 15:56:06 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 16:06:54 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 16:09:33 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 16:12:36 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 16:32:27 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 16:35:00 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 17:42:36 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 17:45:31 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 17:46:08 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 17:46:49 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 17:50:59 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 17:52:28 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 17:54:41 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 17:58:39 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-28 17:59:15 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 11:57:10 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 12:02:07 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 12:03:24 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 12:04:50 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 12:16:08 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:05:28 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:08:53 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:09:19 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:09:54 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:14:02 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:16:48 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:18:17 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:22:27 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:37:33 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:49:34 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:55:15 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 13:55:52 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 15:00:21 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 15:04:50 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 15:25:10 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 15:25:19 ERROR Executor:91 - Exception in task 7.0 in stage 1.0 (TID 15)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:25:19 ERROR Executor:91 - Exception in task 0.0 in stage 1.0 (TID 8)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:25:19 ERROR Executor:91 - Exception in task 5.0 in stage 1.0 (TID 13)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:25:19 ERROR Executor:91 - Exception in task 1.0 in stage 1.0 (TID 9)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:25:19 ERROR Executor:91 - Exception in task 4.0 in stage 1.0 (TID 12)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:25:19 ERROR Executor:91 - Exception in task 2.0 in stage 1.0 (TID 10)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:25:19 ERROR Executor:91 - Exception in task 3.0 in stage 1.0 (TID 11)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:25:19 ERROR Executor:91 - Exception in task 6.0 in stage 1.0 (TID 14)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:25:19 WARN  TaskSetManager:66 - Lost task 3.0 in stage 1.0 (TID 11, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-06-29 15:25:19 ERROR TaskSetManager:70 - Task 3 in stage 1.0 failed 1 times; aborting job
2019-06-29 15:39:15 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 15:39:20 ERROR Executor:91 - Exception in task 0.0 in stage 1.0 (TID 8)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:39:20 ERROR Executor:91 - Exception in task 2.0 in stage 1.0 (TID 10)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:39:20 ERROR Executor:91 - Exception in task 1.0 in stage 1.0 (TID 9)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:39:20 ERROR Executor:91 - Exception in task 6.0 in stage 1.0 (TID 14)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:39:20 ERROR Executor:91 - Exception in task 4.0 in stage 1.0 (TID 12)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:39:20 ERROR Executor:91 - Exception in task 3.0 in stage 1.0 (TID 11)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:39:20 ERROR Executor:91 - Exception in task 5.0 in stage 1.0 (TID 13)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:39:20 ERROR Executor:91 - Exception in task 7.0 in stage 1.0 (TID 15)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:39:20 WARN  TaskSetManager:66 - Lost task 5.0 in stage 1.0 (TID 13, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-06-29 15:39:20 ERROR TaskSetManager:70 - Task 5 in stage 1.0 failed 1 times; aborting job
2019-06-29 15:39:51 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 15:42:28 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 15:49:43 ERROR Executor:91 - Exception in task 5.0 in stage 1.0 (TID 13)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:49:43 ERROR Executor:91 - Exception in task 3.0 in stage 1.0 (TID 11)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:49:43 ERROR Executor:91 - Exception in task 0.0 in stage 1.0 (TID 8)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:49:43 ERROR Executor:91 - Exception in task 4.0 in stage 1.0 (TID 12)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:49:43 ERROR Executor:91 - Exception in task 1.0 in stage 1.0 (TID 9)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:49:43 ERROR Executor:91 - Exception in task 2.0 in stage 1.0 (TID 10)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:49:43 ERROR Executor:91 - Exception in task 7.0 in stage 1.0 (TID 15)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:49:43 ERROR Executor:91 - Exception in task 6.0 in stage 1.0 (TID 14)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-06-29 15:49:43 WARN  TaskSetManager:66 - Lost task 6.0 in stage 1.0 (TID 14, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 267, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-06-29 15:49:43 ERROR TaskSetManager:70 - Task 6 in stage 1.0 failed 1 times; aborting job
2019-06-29 15:55:35 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 15:59:26 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 16:11:54 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 16:13:02 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 16:40:42 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 16:41:58 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 16:43:19 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 16:48:39 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 16:56:11 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 16:57:32 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 16:58:20 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 17:09:39 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 17:10:21 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 17:12:13 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 17:16:40 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 17:17:14 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 17:18:52 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 17:19:26 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 19:21:28 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 19:22:20 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-06-29 19:22:28 WARN  TaskSetManager:66 - Stage 0 contains a task of very large size (13848 KB). The maximum recommended task size is 100 KB.
2019-07-01 12:44:07 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 12:44:07 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 12:44:08 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 12:44:08 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 12:44:08 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 12:44:08 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 12:44:08 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 12:44:08 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 12:44:08 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.042456851s
2019-07-01 12:44:08 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 12:44:08 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 12:44:08 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018693355s
2019-07-01 12:44:08 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:08 INFO  DistriOptimizer$:408 - [Epoch 1 4/6][Iteration 1][Wall Clock 0.117608372s] Trained 4 records in 0.117608372 seconds. Throughput is 34.011185 records/second. Loss is 6.7510576. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:08 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:08 INFO  DistriOptimizer$:408 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.212149577s] Trained 4 records in 0.094541205 seconds. Throughput is 42.309593 records/second. Loss is 3.686531. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:08 INFO  DistriOptimizer$:452 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.212149577s] Epoch finished. Wall clock time is 224.790598 ms
2019-07-01 12:44:08 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:08 INFO  DistriOptimizer$:408 - [Epoch 2 4/6][Iteration 3][Wall Clock 0.298699345s] Trained 4 records in 0.073908747 seconds. Throughput is 54.120792 records/second. Loss is 3.2960606. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:08 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.396162735s] Trained 4 records in 0.09746339 seconds. Throughput is 41.04105 records/second. Loss is 4.4459896. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 INFO  DistriOptimizer$:452 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.396162735s] Epoch finished. Wall clock time is 411.457328 ms
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 3 4/6][Iteration 5][Wall Clock 0.472574153s] Trained 4 records in 0.061116825 seconds. Throughput is 65.448425 records/second. Loss is 4.56502. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.555168095s] Trained 4 records in 0.082593942 seconds. Throughput is 48.429703 records/second. Loss is 5.047777. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 INFO  DistriOptimizer$:452 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.555168095s] Epoch finished. Wall clock time is 569.292001 ms
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 4 4/6][Iteration 7][Wall Clock 0.691005295s] Trained 4 records in 0.121713294 seconds. Throughput is 32.864117 records/second. Loss is 5.245194. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.772949191s] Trained 4 records in 0.081943896 seconds. Throughput is 48.813885 records/second. Loss is 3.642671. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 INFO  DistriOptimizer$:452 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.772949191s] Epoch finished. Wall clock time is 790.612381 ms
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 5 4/6][Iteration 9][Wall Clock 0.831998836s] Trained 4 records in 0.041386455 seconds. Throughput is 96.64998 records/second. Loss is 3.8901794. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.875506295s] Trained 4 records in 0.043507459 seconds. Throughput is 91.938255 records/second. Loss is 3.0715966. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 INFO  DistriOptimizer$:452 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.875506295s] Epoch finished. Wall clock time is 883.139049 ms
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 6 4/6][Iteration 11][Wall Clock 0.92011963s] Trained 4 records in 0.036980581 seconds. Throughput is 108.16488 records/second. Loss is 3.467238. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 6 8/6][Iteration 12][Wall Clock 0.956631202s] Trained 4 records in 0.036511572 seconds. Throughput is 109.55431 records/second. Loss is 2.7567291. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 INFO  DistriOptimizer$:452 - [Epoch 6 8/6][Iteration 12][Wall Clock 0.956631202s] Epoch finished. Wall clock time is 963.296897 ms
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 7 4/6][Iteration 13][Wall Clock 1.007893401s] Trained 4 records in 0.044596504 seconds. Throughput is 89.69313 records/second. Loss is 2.9161658. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 7 8/6][Iteration 14][Wall Clock 1.055817977s] Trained 4 records in 0.047924576 seconds. Throughput is 83.464485 records/second. Loss is 2.6100128. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 INFO  DistriOptimizer$:452 - [Epoch 7 8/6][Iteration 14][Wall Clock 1.055817977s] Epoch finished. Wall clock time is 1063.641421 ms
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 8 4/6][Iteration 15][Wall Clock 1.11697706s] Trained 4 records in 0.053335639 seconds. Throughput is 74.99676 records/second. Loss is 2.720472. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 8 8/6][Iteration 16][Wall Clock 1.157161029s] Trained 4 records in 0.040183969 seconds. Throughput is 99.54218 records/second. Loss is 1.9259652. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 INFO  DistriOptimizer$:452 - [Epoch 8 8/6][Iteration 16][Wall Clock 1.157161029s] Epoch finished. Wall clock time is 1165.745115 ms
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 9 4/6][Iteration 17][Wall Clock 1.19871692s] Trained 4 records in 0.032971805 seconds. Throughput is 121.315765 records/second. Loss is 1.6525071. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 9 8/6][Iteration 18][Wall Clock 1.231764791s] Trained 4 records in 0.033047871 seconds. Throughput is 121.036545 records/second. Loss is 1.0160949. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 INFO  DistriOptimizer$:452 - [Epoch 9 8/6][Iteration 18][Wall Clock 1.231764791s] Epoch finished. Wall clock time is 1238.213951 ms
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 10 4/6][Iteration 19][Wall Clock 1.275353699s] Trained 4 records in 0.037139748 seconds. Throughput is 107.701324 records/second. Loss is 2.0749898. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:09 INFO  DistriOptimizer$:408 - [Epoch 10 8/6][Iteration 20][Wall Clock 1.310274309s] Trained 4 records in 0.03492061 seconds. Throughput is 114.54553 records/second. Loss is 1.7465191. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:09 INFO  DistriOptimizer$:452 - [Epoch 10 8/6][Iteration 20][Wall Clock 1.310274309s] Epoch finished. Wall clock time is 1315.890577 ms
2019-07-01 12:44:09 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 11 4/6][Iteration 21][Wall Clock 1.355948365s] Trained 4 records in 0.040057788 seconds. Throughput is 99.855736 records/second. Loss is 1.3629005. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 11 8/6][Iteration 22][Wall Clock 1.385004048s] Trained 4 records in 0.029055683 seconds. Throughput is 137.6667 records/second. Loss is 1.3174319. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 11 8/6][Iteration 22][Wall Clock 1.385004048s] Epoch finished. Wall clock time is 1390.0744 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 12 4/6][Iteration 23][Wall Clock 1.440781439s] Trained 4 records in 0.050707039 seconds. Throughput is 78.884514 records/second. Loss is 1.405667. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 12 8/6][Iteration 24][Wall Clock 1.475540801s] Trained 4 records in 0.034759362 seconds. Throughput is 115.07691 records/second. Loss is 1.2940003. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 12 8/6][Iteration 24][Wall Clock 1.475540801s] Epoch finished. Wall clock time is 1479.700269 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 13 4/6][Iteration 25][Wall Clock 1.521997078s] Trained 4 records in 0.042296809 seconds. Throughput is 94.569786 records/second. Loss is 0.93426853. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 13 8/6][Iteration 26][Wall Clock 1.573326606s] Trained 4 records in 0.051329528 seconds. Throughput is 77.92786 records/second. Loss is 1.207364. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 13 8/6][Iteration 26][Wall Clock 1.573326606s] Epoch finished. Wall clock time is 1578.818449 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 14 4/6][Iteration 27][Wall Clock 1.609834192s] Trained 4 records in 0.031015743 seconds. Throughput is 128.96677 records/second. Loss is 1.2839781. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 14 8/6][Iteration 28][Wall Clock 1.635728115s] Trained 4 records in 0.025893923 seconds. Throughput is 154.4764 records/second. Loss is 0.9890786. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 14 8/6][Iteration 28][Wall Clock 1.635728115s] Epoch finished. Wall clock time is 1640.270585 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 15 4/6][Iteration 29][Wall Clock 1.673325908s] Trained 4 records in 0.033055323 seconds. Throughput is 121.009254 records/second. Loss is 1.317336. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 15 8/6][Iteration 30][Wall Clock 1.712002602s] Trained 4 records in 0.038676694 seconds. Throughput is 103.421455 records/second. Loss is 0.67302626. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 15 8/6][Iteration 30][Wall Clock 1.712002602s] Epoch finished. Wall clock time is 1716.645299 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 16 4/6][Iteration 31][Wall Clock 1.754163502s] Trained 4 records in 0.037518203 seconds. Throughput is 106.614914 records/second. Loss is 0.64287126. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 16 8/6][Iteration 32][Wall Clock 1.787670497s] Trained 4 records in 0.033506995 seconds. Throughput is 119.37807 records/second. Loss is 0.2982048. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 16 8/6][Iteration 32][Wall Clock 1.787670497s] Epoch finished. Wall clock time is 1796.137742 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 17 4/6][Iteration 33][Wall Clock 1.827869396s] Trained 4 records in 0.031731654 seconds. Throughput is 126.05709 records/second. Loss is 0.63080275. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 17 8/6][Iteration 34][Wall Clock 1.869516693s] Trained 4 records in 0.041647297 seconds. Throughput is 96.04465 records/second. Loss is 0.7084405. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 17 8/6][Iteration 34][Wall Clock 1.869516693s] Epoch finished. Wall clock time is 1873.923731 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 18 4/6][Iteration 35][Wall Clock 1.909173724s] Trained 4 records in 0.035249993 seconds. Throughput is 113.4752 records/second. Loss is 0.57048696. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 18 8/6][Iteration 36][Wall Clock 1.948304935s] Trained 4 records in 0.039131211 seconds. Throughput is 102.2202 records/second. Loss is 0.5145336. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 18 8/6][Iteration 36][Wall Clock 1.948304935s] Epoch finished. Wall clock time is 1954.520347 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 19 4/6][Iteration 37][Wall Clock 1.989728807s] Trained 4 records in 0.03520846 seconds. Throughput is 113.60906 records/second. Loss is 0.39129397. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 19 8/6][Iteration 38][Wall Clock 2.022251042s] Trained 4 records in 0.032522235 seconds. Throughput is 122.992775 records/second. Loss is 0.61075044. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 19 8/6][Iteration 38][Wall Clock 2.022251042s] Epoch finished. Wall clock time is 2028.501155 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 20 4/6][Iteration 39][Wall Clock 2.053764601s] Trained 4 records in 0.025263446 seconds. Throughput is 158.33153 records/second. Loss is 0.56940925. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 20 8/6][Iteration 40][Wall Clock 2.084066873s] Trained 4 records in 0.030302272 seconds. Throughput is 132.00331 records/second. Loss is 0.42951494. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 20 8/6][Iteration 40][Wall Clock 2.084066873s] Epoch finished. Wall clock time is 2088.39597 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 21 4/6][Iteration 41][Wall Clock 2.120548158s] Trained 4 records in 0.032152188 seconds. Throughput is 124.40833 records/second. Loss is 0.72287476. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 21 8/6][Iteration 42][Wall Clock 2.153427987s] Trained 4 records in 0.032879829 seconds. Throughput is 121.655136 records/second. Loss is 0.15078622. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 21 8/6][Iteration 42][Wall Clock 2.153427987s] Epoch finished. Wall clock time is 2157.938234 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 22 4/6][Iteration 43][Wall Clock 2.192937581s] Trained 4 records in 0.034999347 seconds. Throughput is 114.28784 records/second. Loss is 0.45746282. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 22 8/6][Iteration 44][Wall Clock 2.223747475s] Trained 4 records in 0.030809894 seconds. Throughput is 129.82843 records/second. Loss is 0.36953157. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 22 8/6][Iteration 44][Wall Clock 2.223747475s] Epoch finished. Wall clock time is 2229.388928 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 23 4/6][Iteration 45][Wall Clock 2.253864891s] Trained 4 records in 0.024475963 seconds. Throughput is 163.42564 records/second. Loss is 0.41946793. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 23 8/6][Iteration 46][Wall Clock 2.292257638s] Trained 4 records in 0.038392747 seconds. Throughput is 104.18634 records/second. Loss is 0.60739297. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:10 INFO  DistriOptimizer$:452 - [Epoch 23 8/6][Iteration 46][Wall Clock 2.292257638s] Epoch finished. Wall clock time is 2296.36664 ms
2019-07-01 12:44:10 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:10 INFO  DistriOptimizer$:408 - [Epoch 24 4/6][Iteration 47][Wall Clock 2.323252279s] Trained 4 records in 0.026885639 seconds. Throughput is 148.7783 records/second. Loss is 0.5095375. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 24 8/6][Iteration 48][Wall Clock 2.355532912s] Trained 4 records in 0.032280633 seconds. Throughput is 123.913315 records/second. Loss is 0.30969563. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 24 8/6][Iteration 48][Wall Clock 2.355532912s] Epoch finished. Wall clock time is 2359.271806 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 25 4/6][Iteration 49][Wall Clock 2.399399813s] Trained 4 records in 0.040128007 seconds. Throughput is 99.681 records/second. Loss is 0.5392757. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 25 8/6][Iteration 50][Wall Clock 2.428215779s] Trained 4 records in 0.028815966 seconds. Throughput is 138.81194 records/second. Loss is 0.2583067. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 25 8/6][Iteration 50][Wall Clock 2.428215779s] Epoch finished. Wall clock time is 2432.29589 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 26 4/6][Iteration 51][Wall Clock 2.458820645s] Trained 4 records in 0.026524755 seconds. Throughput is 150.80252 records/second. Loss is 0.2667905. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 26 8/6][Iteration 52][Wall Clock 2.481675826s] Trained 4 records in 0.022855181 seconds. Throughput is 175.01501 records/second. Loss is 0.24171786. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 26 8/6][Iteration 52][Wall Clock 2.481675826s] Epoch finished. Wall clock time is 2485.496607 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 27 4/6][Iteration 53][Wall Clock 2.514854602s] Trained 4 records in 0.029357995 seconds. Throughput is 136.24908 records/second. Loss is 0.40143096. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 27 8/6][Iteration 54][Wall Clock 2.541727575s] Trained 4 records in 0.026872973 seconds. Throughput is 148.84843 records/second. Loss is 0.29227328. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 27 8/6][Iteration 54][Wall Clock 2.541727575s] Epoch finished. Wall clock time is 2545.246871 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 28 4/6][Iteration 55][Wall Clock 2.576040329s] Trained 4 records in 0.030793458 seconds. Throughput is 129.89772 records/second. Loss is 0.17895024. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 28 8/6][Iteration 56][Wall Clock 2.603662777s] Trained 4 records in 0.027622448 seconds. Throughput is 144.80975 records/second. Loss is 0.42798334. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 28 8/6][Iteration 56][Wall Clock 2.603662777s] Epoch finished. Wall clock time is 2607.693932 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 29 4/6][Iteration 57][Wall Clock 2.633987198s] Trained 4 records in 0.026293266 seconds. Throughput is 152.1302 records/second. Loss is 0.33855188. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 29 8/6][Iteration 58][Wall Clock 2.658707181s] Trained 4 records in 0.024719983 seconds. Throughput is 161.81241 records/second. Loss is 0.084748015. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 29 8/6][Iteration 58][Wall Clock 2.658707181s] Epoch finished. Wall clock time is 2662.440983 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 30 4/6][Iteration 59][Wall Clock 2.696423949s] Trained 4 records in 0.033982966 seconds. Throughput is 117.70603 records/second. Loss is 0.25386038. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 30 8/6][Iteration 60][Wall Clock 2.744571482s] Trained 4 records in 0.048147533 seconds. Throughput is 83.07799 records/second. Loss is 0.24585463. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 30 8/6][Iteration 60][Wall Clock 2.744571482s] Epoch finished. Wall clock time is 2748.847156 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 31 4/6][Iteration 61][Wall Clock 2.774035118s] Trained 4 records in 0.025187962 seconds. Throughput is 158.80602 records/second. Loss is 0.2938996. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 31 8/6][Iteration 62][Wall Clock 2.800073514s] Trained 4 records in 0.026038396 seconds. Throughput is 153.6193 records/second. Loss is 0.22850612. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 31 8/6][Iteration 62][Wall Clock 2.800073514s] Epoch finished. Wall clock time is 2803.412042 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 32 4/6][Iteration 63][Wall Clock 2.831128976s] Trained 4 records in 0.027716934 seconds. Throughput is 144.3161 records/second. Loss is 0.34473205. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 32 8/6][Iteration 64][Wall Clock 2.856299043s] Trained 4 records in 0.025170067 seconds. Throughput is 158.91893 records/second. Loss is 0.27259442. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 32 8/6][Iteration 64][Wall Clock 2.856299043s] Epoch finished. Wall clock time is 2860.143754 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 33 4/6][Iteration 65][Wall Clock 2.883621878s] Trained 4 records in 0.023478124 seconds. Throughput is 170.37137 records/second. Loss is 0.21026993. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 33 8/6][Iteration 66][Wall Clock 2.909751229s] Trained 4 records in 0.026129351 seconds. Throughput is 153.08455 records/second. Loss is 0.1843113. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 33 8/6][Iteration 66][Wall Clock 2.909751229s] Epoch finished. Wall clock time is 2913.142256 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 34 4/6][Iteration 67][Wall Clock 2.937761236s] Trained 4 records in 0.02461898 seconds. Throughput is 162.47627 records/second. Loss is 0.28116196. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 34 8/6][Iteration 68][Wall Clock 2.966017176s] Trained 4 records in 0.02825594 seconds. Throughput is 141.56316 records/second. Loss is 0.13033749. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 34 8/6][Iteration 68][Wall Clock 2.966017176s] Epoch finished. Wall clock time is 2971.338524 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 35 4/6][Iteration 69][Wall Clock 2.994109301s] Trained 4 records in 0.022770777 seconds. Throughput is 175.66374 records/second. Loss is 0.2381399. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 35 8/6][Iteration 70][Wall Clock 3.018811639s] Trained 4 records in 0.024702338 seconds. Throughput is 161.928 records/second. Loss is 0.27817512. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 35 8/6][Iteration 70][Wall Clock 3.018811639s] Epoch finished. Wall clock time is 3023.169024 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 36 4/6][Iteration 71][Wall Clock 3.044825364s] Trained 4 records in 0.02165634 seconds. Throughput is 184.70341 records/second. Loss is 0.20127565. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 36 8/6][Iteration 72][Wall Clock 3.068358442s] Trained 4 records in 0.023533078 seconds. Throughput is 169.97351 records/second. Loss is 0.08104947. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 36 8/6][Iteration 72][Wall Clock 3.068358442s] Epoch finished. Wall clock time is 3073.193973 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 37 4/6][Iteration 73][Wall Clock 3.09784464s] Trained 4 records in 0.024650667 seconds. Throughput is 162.26741 records/second. Loss is 0.07937978. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 37 8/6][Iteration 74][Wall Clock 3.132907231s] Trained 4 records in 0.035062591 seconds. Throughput is 114.081696 records/second. Loss is 0.24120405. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 37 8/6][Iteration 74][Wall Clock 3.132907231s] Epoch finished. Wall clock time is 3136.361621 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 38 4/6][Iteration 75][Wall Clock 3.166655699s] Trained 4 records in 0.030294078 seconds. Throughput is 132.03902 records/second. Loss is 0.15943256. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 38 8/6][Iteration 76][Wall Clock 3.188388059s] Trained 4 records in 0.02173236 seconds. Throughput is 184.05733 records/second. Loss is 0.11741531. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 38 8/6][Iteration 76][Wall Clock 3.188388059s] Epoch finished. Wall clock time is 3193.218918 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 39 4/6][Iteration 77][Wall Clock 3.220071252s] Trained 4 records in 0.026852334 seconds. Throughput is 148.96284 records/second. Loss is 0.19867943. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 39 8/6][Iteration 78][Wall Clock 3.248896835s] Trained 4 records in 0.028825583 seconds. Throughput is 138.76562 records/second. Loss is 0.11017694. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 39 8/6][Iteration 78][Wall Clock 3.248896835s] Epoch finished. Wall clock time is 3252.064887 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 40 4/6][Iteration 79][Wall Clock 3.274987598s] Trained 4 records in 0.022922711 seconds. Throughput is 174.49942 records/second. Loss is 0.14057676. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 40 8/6][Iteration 80][Wall Clock 3.295039813s] Trained 4 records in 0.020052215 seconds. Throughput is 199.4792 records/second. Loss is 0.21699357. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:11 INFO  DistriOptimizer$:452 - [Epoch 40 8/6][Iteration 80][Wall Clock 3.295039813s] Epoch finished. Wall clock time is 3299.113346 ms
2019-07-01 12:44:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:11 INFO  DistriOptimizer$:408 - [Epoch 41 4/6][Iteration 81][Wall Clock 3.32290158s] Trained 4 records in 0.023788234 seconds. Throughput is 168.15036 records/second. Loss is 0.150507. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 41 8/6][Iteration 82][Wall Clock 3.34992773s] Trained 4 records in 0.02702615 seconds. Throughput is 148.0048 records/second. Loss is 0.10900726. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 INFO  DistriOptimizer$:452 - [Epoch 41 8/6][Iteration 82][Wall Clock 3.34992773s] Epoch finished. Wall clock time is 3353.208859 ms
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 42 4/6][Iteration 83][Wall Clock 3.379128188s] Trained 4 records in 0.025919329 seconds. Throughput is 154.32498 records/second. Loss is 0.20732115. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 42 8/6][Iteration 84][Wall Clock 3.403926496s] Trained 4 records in 0.024798308 seconds. Throughput is 161.30133 records/second. Loss is 0.19957407. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 INFO  DistriOptimizer$:452 - [Epoch 42 8/6][Iteration 84][Wall Clock 3.403926496s] Epoch finished. Wall clock time is 3407.429411 ms
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 43 4/6][Iteration 85][Wall Clock 3.432056045s] Trained 4 records in 0.024626634 seconds. Throughput is 162.42578 records/second. Loss is 0.115540914. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 43 8/6][Iteration 86][Wall Clock 3.455391261s] Trained 4 records in 0.023335216 seconds. Throughput is 171.41473 records/second. Loss is 0.12912309. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 INFO  DistriOptimizer$:452 - [Epoch 43 8/6][Iteration 86][Wall Clock 3.455391261s] Epoch finished. Wall clock time is 3458.431578 ms
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 44 4/6][Iteration 87][Wall Clock 3.480074061s] Trained 4 records in 0.021642483 seconds. Throughput is 184.82167 records/second. Loss is 0.15583429. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 44 8/6][Iteration 88][Wall Clock 3.502298412s] Trained 4 records in 0.022224351 seconds. Throughput is 179.98276 records/second. Loss is 0.1402076. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 INFO  DistriOptimizer$:452 - [Epoch 44 8/6][Iteration 88][Wall Clock 3.502298412s] Epoch finished. Wall clock time is 3505.678996 ms
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 45 4/6][Iteration 89][Wall Clock 3.532402775s] Trained 4 records in 0.026723779 seconds. Throughput is 149.67943 records/second. Loss is 0.1399385. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 45 8/6][Iteration 90][Wall Clock 3.561823233s] Trained 4 records in 0.029420458 seconds. Throughput is 135.95981 records/second. Loss is 0.13494822. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 INFO  DistriOptimizer$:452 - [Epoch 45 8/6][Iteration 90][Wall Clock 3.561823233s] Epoch finished. Wall clock time is 3565.323304 ms
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 46 4/6][Iteration 91][Wall Clock 3.613652454s] Trained 4 records in 0.04832915 seconds. Throughput is 82.765785 records/second. Loss is 0.14093082. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 46 8/6][Iteration 92][Wall Clock 3.641967581s] Trained 4 records in 0.028315127 seconds. Throughput is 141.26724 records/second. Loss is 0.12175271. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 INFO  DistriOptimizer$:452 - [Epoch 46 8/6][Iteration 92][Wall Clock 3.641967581s] Epoch finished. Wall clock time is 3646.916656 ms
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 47 4/6][Iteration 93][Wall Clock 3.674083251s] Trained 4 records in 0.027166595 seconds. Throughput is 147.23965 records/second. Loss is 0.109007634. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 47 8/6][Iteration 94][Wall Clock 3.698365089s] Trained 4 records in 0.024281838 seconds. Throughput is 164.73216 records/second. Loss is 0.14681341. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 INFO  DistriOptimizer$:452 - [Epoch 47 8/6][Iteration 94][Wall Clock 3.698365089s] Epoch finished. Wall clock time is 3702.492629 ms
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 48 4/6][Iteration 95][Wall Clock 3.728676421s] Trained 4 records in 0.026183792 seconds. Throughput is 152.76627 records/second. Loss is 0.15720758. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 48 8/6][Iteration 96][Wall Clock 3.749771053s] Trained 4 records in 0.021094632 seconds. Throughput is 189.6217 records/second. Loss is 0.16702288. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 INFO  DistriOptimizer$:452 - [Epoch 48 8/6][Iteration 96][Wall Clock 3.749771053s] Epoch finished. Wall clock time is 3753.234177 ms
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 49 4/6][Iteration 97][Wall Clock 3.782911848s] Trained 4 records in 0.029677671 seconds. Throughput is 134.78146 records/second. Loss is 0.13109298. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 49 8/6][Iteration 98][Wall Clock 3.809054799s] Trained 4 records in 0.026142951 seconds. Throughput is 153.00491 records/second. Loss is 0.1334256. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 INFO  DistriOptimizer$:452 - [Epoch 49 8/6][Iteration 98][Wall Clock 3.809054799s] Epoch finished. Wall clock time is 3813.201213 ms
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 50 4/6][Iteration 99][Wall Clock 3.84075221s] Trained 4 records in 0.027550997 seconds. Throughput is 145.1853 records/second. Loss is 0.121560656. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:44:12 INFO  DistriOptimizer$:408 - [Epoch 50 8/6][Iteration 100][Wall Clock 3.864252708s] Trained 4 records in 0.023500498 seconds. Throughput is 170.20915 records/second. Loss is 0.1557612. Lineara95e2639's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:44:12 INFO  DistriOptimizer$:452 - [Epoch 50 8/6][Iteration 100][Wall Clock 3.864252708s] Epoch finished. Wall clock time is 3869.161725 ms
2019-07-01 12:45:48 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 12:45:48 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 12:45:48 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 12:45:48 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 12:45:48 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 12:45:48 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 12:45:48 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 12:45:48 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 12:45:49 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.042800941s
2019-07-01 12:45:49 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 12:45:49 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 12:45:49 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.016926773s
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 1 4/6][Iteration 1][Wall Clock 0.132208424s] Trained 4 records in 0.132208424 seconds. Throughput is 30.255259 records/second. Loss is 17.57066. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.230179287s] Trained 4 records in 0.097970863 seconds. Throughput is 40.828465 records/second. Loss is 8.411466. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 INFO  DistriOptimizer$:452 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.230179287s] Epoch finished. Wall clock time is 245.413134 ms
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 2 4/6][Iteration 3][Wall Clock 0.31937654s] Trained 4 records in 0.073963406 seconds. Throughput is 54.0808 records/second. Loss is 16.51025. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.39649254s] Trained 4 records in 0.077116 seconds. Throughput is 51.86991 records/second. Loss is 7.916806. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 INFO  DistriOptimizer$:452 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.39649254s] Epoch finished. Wall clock time is 409.263714 ms
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 3 4/6][Iteration 5][Wall Clock 0.472150077s] Trained 4 records in 0.062886363 seconds. Throughput is 63.606792 records/second. Loss is 7.4914455. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.525945175s] Trained 4 records in 0.053795098 seconds. Throughput is 74.35622 records/second. Loss is 11.85862. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 INFO  DistriOptimizer$:452 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.525945175s] Epoch finished. Wall clock time is 535.858118 ms
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 4 4/6][Iteration 7][Wall Clock 0.65495184s] Trained 4 records in 0.119093722 seconds. Throughput is 33.586994 records/second. Loss is 4.3831863. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.737379401s] Trained 4 records in 0.082427561 seconds. Throughput is 48.52746 records/second. Loss is 7.9942174. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 INFO  DistriOptimizer$:452 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.737379401s] Epoch finished. Wall clock time is 747.055728 ms
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 5 4/6][Iteration 9][Wall Clock 0.78661993s] Trained 4 records in 0.039564202 seconds. Throughput is 101.101494 records/second. Loss is 7.2761073. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.824390481s] Trained 4 records in 0.037770551 seconds. Throughput is 105.90261 records/second. Loss is 4.646113. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 INFO  DistriOptimizer$:452 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.824390481s] Epoch finished. Wall clock time is 830.016028 ms
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 6 4/6][Iteration 11][Wall Clock 0.875158205s] Trained 4 records in 0.045142177 seconds. Throughput is 88.60893 records/second. Loss is 4.9673266. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:49 INFO  DistriOptimizer$:408 - [Epoch 6 8/6][Iteration 12][Wall Clock 0.906092245s] Trained 4 records in 0.03093404 seconds. Throughput is 129.30739 records/second. Loss is 4.6912. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:49 INFO  DistriOptimizer$:452 - [Epoch 6 8/6][Iteration 12][Wall Clock 0.906092245s] Epoch finished. Wall clock time is 911.901162 ms
2019-07-01 12:45:49 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 7 4/6][Iteration 13][Wall Clock 0.947562416s] Trained 4 records in 0.035661254 seconds. Throughput is 112.16656 records/second. Loss is 2.7910442. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 7 8/6][Iteration 14][Wall Clock 0.982284637s] Trained 4 records in 0.034722221 seconds. Throughput is 115.200005 records/second. Loss is 7.777711. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 INFO  DistriOptimizer$:452 - [Epoch 7 8/6][Iteration 14][Wall Clock 0.982284637s] Epoch finished. Wall clock time is 988.818131 ms
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 8 4/6][Iteration 15][Wall Clock 1.058793114s] Trained 4 records in 0.069974983 seconds. Throughput is 57.16329 records/second. Loss is 5.3358088. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 8 8/6][Iteration 16][Wall Clock 1.096614361s] Trained 4 records in 0.037821247 seconds. Throughput is 105.76066 records/second. Loss is 4.376816. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 INFO  DistriOptimizer$:452 - [Epoch 8 8/6][Iteration 16][Wall Clock 1.096614361s] Epoch finished. Wall clock time is 1102.224272 ms
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 9 4/6][Iteration 17][Wall Clock 1.159404893s] Trained 4 records in 0.057180621 seconds. Throughput is 69.95377 records/second. Loss is 4.0028257. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 9 8/6][Iteration 18][Wall Clock 1.193189057s] Trained 4 records in 0.033784164 seconds. Throughput is 118.398674 records/second. Loss is 3.669975. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 INFO  DistriOptimizer$:452 - [Epoch 9 8/6][Iteration 18][Wall Clock 1.193189057s] Epoch finished. Wall clock time is 1202.123002 ms
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 10 4/6][Iteration 19][Wall Clock 1.235141274s] Trained 4 records in 0.033018272 seconds. Throughput is 121.14504 records/second. Loss is 4.7397656. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 10 8/6][Iteration 20][Wall Clock 1.280990124s] Trained 4 records in 0.04584885 seconds. Throughput is 87.243195 records/second. Loss is 2.3218057. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 INFO  DistriOptimizer$:452 - [Epoch 10 8/6][Iteration 20][Wall Clock 1.280990124s] Epoch finished. Wall clock time is 1285.761019 ms
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 11 4/6][Iteration 21][Wall Clock 1.325621825s] Trained 4 records in 0.039860806 seconds. Throughput is 100.3492 records/second. Loss is 2.9024868. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 11 8/6][Iteration 22][Wall Clock 1.365123149s] Trained 4 records in 0.039501324 seconds. Throughput is 101.26243 records/second. Loss is 3.4901783. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 INFO  DistriOptimizer$:452 - [Epoch 11 8/6][Iteration 22][Wall Clock 1.365123149s] Epoch finished. Wall clock time is 1373.632476 ms
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 12 4/6][Iteration 23][Wall Clock 1.407170174s] Trained 4 records in 0.033537698 seconds. Throughput is 119.268776 records/second. Loss is 2.14852. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 12 8/6][Iteration 24][Wall Clock 1.474910615s] Trained 4 records in 0.067740441 seconds. Throughput is 59.048923 records/second. Loss is 2.8017979. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 INFO  DistriOptimizer$:452 - [Epoch 12 8/6][Iteration 24][Wall Clock 1.474910615s] Epoch finished. Wall clock time is 1479.564725 ms
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 13 4/6][Iteration 25][Wall Clock 1.519700164s] Trained 4 records in 0.040135439 seconds. Throughput is 99.662544 records/second. Loss is 1.9490062. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 13 8/6][Iteration 26][Wall Clock 1.555716703s] Trained 4 records in 0.036016539 seconds. Throughput is 111.06009 records/second. Loss is 2.2956116. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 INFO  DistriOptimizer$:452 - [Epoch 13 8/6][Iteration 26][Wall Clock 1.555716703s] Epoch finished. Wall clock time is 1563.90314 ms
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 14 4/6][Iteration 27][Wall Clock 1.609351172s] Trained 4 records in 0.045448032 seconds. Throughput is 88.01261 records/second. Loss is 2.0976899. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 14 8/6][Iteration 28][Wall Clock 1.641592816s] Trained 4 records in 0.032241644 seconds. Throughput is 124.063156 records/second. Loss is 2.4196064. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 INFO  DistriOptimizer$:452 - [Epoch 14 8/6][Iteration 28][Wall Clock 1.641592816s] Epoch finished. Wall clock time is 1646.415403 ms
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 15 4/6][Iteration 29][Wall Clock 1.672222162s] Trained 4 records in 0.025806759 seconds. Throughput is 154.99815 records/second. Loss is 2.2108583. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 15 8/6][Iteration 30][Wall Clock 1.702255828s] Trained 4 records in 0.030033666 seconds. Throughput is 133.18387 records/second. Loss is 1.163824. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 INFO  DistriOptimizer$:452 - [Epoch 15 8/6][Iteration 30][Wall Clock 1.702255828s] Epoch finished. Wall clock time is 1706.008451 ms
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 16 4/6][Iteration 31][Wall Clock 1.739479325s] Trained 4 records in 0.033470874 seconds. Throughput is 119.50689 records/second. Loss is 0.9819503. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 16 8/6][Iteration 32][Wall Clock 1.770231802s] Trained 4 records in 0.030752477 seconds. Throughput is 130.07083 records/second. Loss is 0.51176655. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 INFO  DistriOptimizer$:452 - [Epoch 16 8/6][Iteration 32][Wall Clock 1.770231802s] Epoch finished. Wall clock time is 1775.273124 ms
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 17 4/6][Iteration 33][Wall Clock 1.811687902s] Trained 4 records in 0.036414778 seconds. Throughput is 109.84551 records/second. Loss is 1.2925342. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 17 8/6][Iteration 34][Wall Clock 1.838544099s] Trained 4 records in 0.026856197 seconds. Throughput is 148.94142 records/second. Loss is 1.1708602. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 INFO  DistriOptimizer$:452 - [Epoch 17 8/6][Iteration 34][Wall Clock 1.838544099s] Epoch finished. Wall clock time is 1843.173686 ms
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:50 INFO  DistriOptimizer$:408 - [Epoch 18 4/6][Iteration 35][Wall Clock 1.879404866s] Trained 4 records in 0.03623118 seconds. Throughput is 110.402145 records/second. Loss is 1.0631319. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 18 8/6][Iteration 36][Wall Clock 1.928718084s] Trained 4 records in 0.049313218 seconds. Throughput is 81.11416 records/second. Loss is 1.0402014. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 18 8/6][Iteration 36][Wall Clock 1.928718084s] Epoch finished. Wall clock time is 1934.317294 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 19 4/6][Iteration 37][Wall Clock 1.973882833s] Trained 4 records in 0.039565539 seconds. Throughput is 101.098076 records/second. Loss is 1.4447218. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 19 8/6][Iteration 38][Wall Clock 2.004607093s] Trained 4 records in 0.03072426 seconds. Throughput is 130.19028 records/second. Loss is 0.32084754. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 19 8/6][Iteration 38][Wall Clock 2.004607093s] Epoch finished. Wall clock time is 2011.142208 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 20 4/6][Iteration 39][Wall Clock 2.040725338s] Trained 4 records in 0.02958313 seconds. Throughput is 135.2122 records/second. Loss is 0.81583273. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 20 8/6][Iteration 40][Wall Clock 2.072079461s] Trained 4 records in 0.031354123 seconds. Throughput is 127.574936 records/second. Loss is 1.2338507. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 20 8/6][Iteration 40][Wall Clock 2.072079461s] Epoch finished. Wall clock time is 2077.062583 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 21 4/6][Iteration 41][Wall Clock 2.109168359s] Trained 4 records in 0.032105776 seconds. Throughput is 124.588165 records/second. Loss is 1.13388. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 21 8/6][Iteration 42][Wall Clock 2.139998667s] Trained 4 records in 0.030830308 seconds. Throughput is 129.74246 records/second. Loss is 0.24030378. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 21 8/6][Iteration 42][Wall Clock 2.139998667s] Epoch finished. Wall clock time is 2143.689609 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 22 4/6][Iteration 43][Wall Clock 2.179988394s] Trained 4 records in 0.036298785 seconds. Throughput is 110.196526 records/second. Loss is 1.077453. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 22 8/6][Iteration 44][Wall Clock 2.212754824s] Trained 4 records in 0.03276643 seconds. Throughput is 122.07616 records/second. Loss is 0.94438267. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 22 8/6][Iteration 44][Wall Clock 2.212754824s] Epoch finished. Wall clock time is 2219.183803 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 23 4/6][Iteration 45][Wall Clock 2.249683699s] Trained 4 records in 0.030499896 seconds. Throughput is 131.148 records/second. Loss is 0.8495885. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 23 8/6][Iteration 46][Wall Clock 2.279831336s] Trained 4 records in 0.030147637 seconds. Throughput is 132.68039 records/second. Loss is 0.5278238. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 23 8/6][Iteration 46][Wall Clock 2.279831336s] Epoch finished. Wall clock time is 2283.767423 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 24 4/6][Iteration 47][Wall Clock 2.314076782s] Trained 4 records in 0.030309359 seconds. Throughput is 131.97244 records/second. Loss is 0.17434067. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 24 8/6][Iteration 48][Wall Clock 2.356018374s] Trained 4 records in 0.041941592 seconds. Throughput is 95.37073 records/second. Loss is 0.5374029. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 24 8/6][Iteration 48][Wall Clock 2.356018374s] Epoch finished. Wall clock time is 2360.176715 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 25 4/6][Iteration 49][Wall Clock 2.387910139s] Trained 4 records in 0.027733424 seconds. Throughput is 144.2303 records/second. Loss is 0.7286506. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 25 8/6][Iteration 50][Wall Clock 2.408949564s] Trained 4 records in 0.021039425 seconds. Throughput is 190.11926 records/second. Loss is 0.44785598. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 25 8/6][Iteration 50][Wall Clock 2.408949564s] Epoch finished. Wall clock time is 2413.186503 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 26 4/6][Iteration 51][Wall Clock 2.437168167s] Trained 4 records in 0.023981664 seconds. Throughput is 166.7941 records/second. Loss is 0.43405142. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 26 8/6][Iteration 52][Wall Clock 2.457784865s] Trained 4 records in 0.020616698 seconds. Throughput is 194.0175 records/second. Loss is 0.7431228. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 26 8/6][Iteration 52][Wall Clock 2.457784865s] Epoch finished. Wall clock time is 2461.134663 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 27 4/6][Iteration 53][Wall Clock 2.485539356s] Trained 4 records in 0.024404693 seconds. Throughput is 163.9029 records/second. Loss is 0.42761728. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 27 8/6][Iteration 54][Wall Clock 2.509166804s] Trained 4 records in 0.023627448 seconds. Throughput is 169.29462 records/second. Loss is 0.6536433. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 27 8/6][Iteration 54][Wall Clock 2.509166804s] Epoch finished. Wall clock time is 2512.785589 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 28 4/6][Iteration 55][Wall Clock 2.534663906s] Trained 4 records in 0.021878317 seconds. Throughput is 182.82942 records/second. Loss is 0.38177451. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 28 8/6][Iteration 56][Wall Clock 2.559766437s] Trained 4 records in 0.025102531 seconds. Throughput is 159.34648 records/second. Loss is 0.36425868. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 28 8/6][Iteration 56][Wall Clock 2.559766437s] Epoch finished. Wall clock time is 2562.823443 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 29 4/6][Iteration 57][Wall Clock 2.589247067s] Trained 4 records in 0.026423624 seconds. Throughput is 151.37968 records/second. Loss is 0.50161666. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 29 8/6][Iteration 58][Wall Clock 2.61696801s] Trained 4 records in 0.027720943 seconds. Throughput is 144.29524 records/second. Loss is 0.4727782. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 29 8/6][Iteration 58][Wall Clock 2.61696801s] Epoch finished. Wall clock time is 2620.299449 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 30 4/6][Iteration 59][Wall Clock 2.648295837s] Trained 4 records in 0.027996388 seconds. Throughput is 142.87558 records/second. Loss is 0.23934284. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 30 8/6][Iteration 60][Wall Clock 2.675216894s] Trained 4 records in 0.026921057 seconds. Throughput is 148.58258 records/second. Loss is 0.32498893. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 30 8/6][Iteration 60][Wall Clock 2.675216894s] Epoch finished. Wall clock time is 2678.696906 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 31 4/6][Iteration 61][Wall Clock 2.708421063s] Trained 4 records in 0.029724157 seconds. Throughput is 134.57068 records/second. Loss is 0.3233379. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 31 8/6][Iteration 62][Wall Clock 2.738863671s] Trained 4 records in 0.030442608 seconds. Throughput is 131.39479 records/second. Loss is 0.38831758. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 31 8/6][Iteration 62][Wall Clock 2.738863671s] Epoch finished. Wall clock time is 2742.978882 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 32 4/6][Iteration 63][Wall Clock 2.777512066s] Trained 4 records in 0.034533184 seconds. Throughput is 115.83062 records/second. Loss is 0.4646718. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 32 8/6][Iteration 64][Wall Clock 2.799594026s] Trained 4 records in 0.02208196 seconds. Throughput is 181.14334 records/second. Loss is 0.19768082. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 32 8/6][Iteration 64][Wall Clock 2.799594026s] Epoch finished. Wall clock time is 2803.247719 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 33 4/6][Iteration 65][Wall Clock 2.82388642s] Trained 4 records in 0.020638701 seconds. Throughput is 193.81065 records/second. Loss is 0.43660927. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 33 8/6][Iteration 66][Wall Clock 2.846991074s] Trained 4 records in 0.023104654 seconds. Throughput is 173.12529 records/second. Loss is 0.28082207. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 33 8/6][Iteration 66][Wall Clock 2.846991074s] Epoch finished. Wall clock time is 2849.979045 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 34 4/6][Iteration 67][Wall Clock 2.869234715s] Trained 4 records in 0.01925567 seconds. Throughput is 207.73102 records/second. Loss is 0.119368635. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 34 8/6][Iteration 68][Wall Clock 2.892491022s] Trained 4 records in 0.023256307 seconds. Throughput is 171.99635 records/second. Loss is 0.31939346. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 INFO  DistriOptimizer$:452 - [Epoch 34 8/6][Iteration 68][Wall Clock 2.892491022s] Epoch finished. Wall clock time is 2895.814328 ms
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:51 INFO  DistriOptimizer$:408 - [Epoch 35 4/6][Iteration 69][Wall Clock 2.918147431s] Trained 4 records in 0.022333103 seconds. Throughput is 179.10632 records/second. Loss is 0.2655633. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 35 8/6][Iteration 70][Wall Clock 2.943987688s] Trained 4 records in 0.025840257 seconds. Throughput is 154.79723 records/second. Loss is 0.26045966. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 35 8/6][Iteration 70][Wall Clock 2.943987688s] Epoch finished. Wall clock time is 2948.319906 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 36 4/6][Iteration 71][Wall Clock 2.971308932s] Trained 4 records in 0.022989026 seconds. Throughput is 173.99606 records/second. Loss is 0.3709032. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 36 8/6][Iteration 72][Wall Clock 2.99876534s] Trained 4 records in 0.027456408 seconds. Throughput is 145.68549 records/second. Loss is 0.35502803. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 36 8/6][Iteration 72][Wall Clock 2.99876534s] Epoch finished. Wall clock time is 3001.814738 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 37 4/6][Iteration 73][Wall Clock 3.03015962s] Trained 4 records in 0.028344882 seconds. Throughput is 141.11894 records/second. Loss is 0.29146284. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 37 8/6][Iteration 74][Wall Clock 3.055318004s] Trained 4 records in 0.025158384 seconds. Throughput is 158.99272 records/second. Loss is 0.26175758. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 37 8/6][Iteration 74][Wall Clock 3.055318004s] Epoch finished. Wall clock time is 3059.702654 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 38 4/6][Iteration 75][Wall Clock 3.087062482s] Trained 4 records in 0.027359828 seconds. Throughput is 146.19975 records/second. Loss is 0.27273402. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 38 8/6][Iteration 76][Wall Clock 3.116497712s] Trained 4 records in 0.02943523 seconds. Throughput is 135.89159 records/second. Loss is 0.20963727. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 38 8/6][Iteration 76][Wall Clock 3.116497712s] Epoch finished. Wall clock time is 3120.738594 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 39 4/6][Iteration 77][Wall Clock 3.152399996s] Trained 4 records in 0.031661402 seconds. Throughput is 126.33679 records/second. Loss is 0.30691564. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 39 8/6][Iteration 78][Wall Clock 3.17642091s] Trained 4 records in 0.024020914 seconds. Throughput is 166.52156 records/second. Loss is 0.29964364. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 39 8/6][Iteration 78][Wall Clock 3.17642091s] Epoch finished. Wall clock time is 3181.214952 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 40 4/6][Iteration 79][Wall Clock 3.217837115s] Trained 4 records in 0.036622163 seconds. Throughput is 109.22348 records/second. Loss is 0.21870025. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 40 8/6][Iteration 80][Wall Clock 3.236828106s] Trained 4 records in 0.018990991 seconds. Throughput is 210.62617 records/second. Loss is 0.21333964. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 40 8/6][Iteration 80][Wall Clock 3.236828106s] Epoch finished. Wall clock time is 3240.114966 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 41 4/6][Iteration 81][Wall Clock 3.2604822s] Trained 4 records in 0.020367234 seconds. Throughput is 196.39389 records/second. Loss is 0.16724095. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 41 8/6][Iteration 82][Wall Clock 3.28082383s] Trained 4 records in 0.02034163 seconds. Throughput is 196.64108 records/second. Loss is 0.16209427. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 41 8/6][Iteration 82][Wall Clock 3.28082383s] Epoch finished. Wall clock time is 3284.530223 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 42 4/6][Iteration 83][Wall Clock 3.305814374s] Trained 4 records in 0.021284151 seconds. Throughput is 187.93326 records/second. Loss is 0.19565445. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 42 8/6][Iteration 84][Wall Clock 3.32822621s] Trained 4 records in 0.022411836 seconds. Throughput is 178.47713 records/second. Loss is 0.27334565. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 42 8/6][Iteration 84][Wall Clock 3.32822621s] Epoch finished. Wall clock time is 3331.11044 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 43 4/6][Iteration 85][Wall Clock 3.351261215s] Trained 4 records in 0.020150775 seconds. Throughput is 198.50352 records/second. Loss is 0.22918499. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 43 8/6][Iteration 86][Wall Clock 3.372257386s] Trained 4 records in 0.020996171 seconds. Throughput is 190.51094 records/second. Loss is 0.20988728. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 43 8/6][Iteration 86][Wall Clock 3.372257386s] Epoch finished. Wall clock time is 3375.044802 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 44 4/6][Iteration 87][Wall Clock 3.396095683s] Trained 4 records in 0.021050881 seconds. Throughput is 190.0158 records/second. Loss is 0.20735338. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 44 8/6][Iteration 88][Wall Clock 3.415686962s] Trained 4 records in 0.019591279 seconds. Throughput is 204.17247 records/second. Loss is 0.20013322. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 44 8/6][Iteration 88][Wall Clock 3.415686962s] Epoch finished. Wall clock time is 3419.008288 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 45 4/6][Iteration 89][Wall Clock 3.438528166s] Trained 4 records in 0.019519878 seconds. Throughput is 204.91931 records/second. Loss is 0.18553871. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 45 8/6][Iteration 90][Wall Clock 3.461974326s] Trained 4 records in 0.02344616 seconds. Throughput is 170.60364 records/second. Loss is 0.19670379. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 45 8/6][Iteration 90][Wall Clock 3.461974326s] Epoch finished. Wall clock time is 3466.975404 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 46 4/6][Iteration 91][Wall Clock 3.490353697s] Trained 4 records in 0.023378293 seconds. Throughput is 171.09889 records/second. Loss is 0.18326777. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 46 8/6][Iteration 92][Wall Clock 3.513097111s] Trained 4 records in 0.022743414 seconds. Throughput is 175.87509 records/second. Loss is 0.1637299. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 46 8/6][Iteration 92][Wall Clock 3.513097111s] Epoch finished. Wall clock time is 3516.904654 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 47 4/6][Iteration 93][Wall Clock 3.537658525s] Trained 4 records in 0.020753871 seconds. Throughput is 192.73512 records/second. Loss is 0.19482338. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 47 8/6][Iteration 94][Wall Clock 3.561883131s] Trained 4 records in 0.024224606 seconds. Throughput is 165.12137 records/second. Loss is 0.19463718. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 47 8/6][Iteration 94][Wall Clock 3.561883131s] Epoch finished. Wall clock time is 3565.107231 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 48 4/6][Iteration 95][Wall Clock 3.607081279s] Trained 4 records in 0.041974048 seconds. Throughput is 95.296974 records/second. Loss is 0.23242734. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 48 8/6][Iteration 96][Wall Clock 3.634528154s] Trained 4 records in 0.027446875 seconds. Throughput is 145.73608 records/second. Loss is 0.19022483. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 48 8/6][Iteration 96][Wall Clock 3.634528154s] Epoch finished. Wall clock time is 3638.794892 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 49 4/6][Iteration 97][Wall Clock 3.665953441s] Trained 4 records in 0.027158549 seconds. Throughput is 147.28328 records/second. Loss is 0.13990372. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 49 8/6][Iteration 98][Wall Clock 3.6899392s] Trained 4 records in 0.023985759 seconds. Throughput is 166.76562 records/second. Loss is 0.1739966. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 49 8/6][Iteration 98][Wall Clock 3.6899392s] Epoch finished. Wall clock time is 3694.639139 ms
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 50 4/6][Iteration 99][Wall Clock 3.715324282s] Trained 4 records in 0.020685143 seconds. Throughput is 193.3755 records/second. Loss is 0.19562697. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 12:45:52 INFO  DistriOptimizer$:408 - [Epoch 50 8/6][Iteration 100][Wall Clock 3.735125517s] Trained 4 records in 0.019801235 seconds. Throughput is 202.0076 records/second. Loss is 0.18847722. Linear840ce7e6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 12:45:52 INFO  DistriOptimizer$:452 - [Epoch 50 8/6][Iteration 100][Wall Clock 3.735125517s] Epoch finished. Wall clock time is 3738.315536 ms
2019-07-01 13:20:04 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:20:04 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:20:05 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:20:05 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:20:05 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:20:05 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:20:05 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:20:05 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:20:05 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.045133841s
2019-07-01 13:20:05 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:20:05 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:20:05 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018476837s
2019-07-01 13:20:05 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 13:20:05 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:20:05 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 13:20:05 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 13:20:47 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:20:47 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:20:48 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:20:48 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:20:48 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:20:48 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:20:48 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:20:48 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:20:48 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.04758292s
2019-07-01 13:20:48 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:20:48 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:20:48 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.019456881s
2019-07-01 13:20:48 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:20:48 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 13:20:48 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:20:48 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 13:20:48 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 13:23:16 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:23:17 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:23:17 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:23:17 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:23:17 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:23:17 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:23:17 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:23:17 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:23:17 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.043636374s
2019-07-01 13:23:17 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:23:17 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:23:18 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.020063572s
2019-07-01 13:23:18 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:23:18 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 13:23:18 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:23:18 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 13:23:18 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 13:26:21 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:26:21 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:26:22 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:26:22 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:26:22 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:26:22 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:26:22 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:26:22 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:26:22 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.04623164s
2019-07-01 13:26:22 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:26:22 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:26:22 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.021779609s
2019-07-01 13:26:22 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 2
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 13:26:22 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 2
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:26:22 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 13:26:22 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 2
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 2
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 13:26:51 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:26:51 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:26:52 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:26:52 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:26:52 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:26:52 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:26:52 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:26:52 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:26:52 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.051538418s
2019-07-01 13:26:52 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:26:52 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:26:52 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017959291s
2019-07-01 13:26:52 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:26:52 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 13:26:52 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:26:52 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 13:26:52 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 13:29:17 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:29:17 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:29:17 ERROR Executor:91 - Exception in task 7.0 in stage 0.0 (TID 7)
java.lang.ClassCastException: com.intel.analytics.bigdl.python.api.Sample cannot be cast to java.lang.String
	at org.apache.spark.bigdl.api.python.BigDLSerDe$SamplePickler.construct(BigDLSerde.scala:224)
	at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)
	at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)
	at net.razorvine.pickle.Unpickler.load(Unpickler.java:99)
	at net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)
	at org.apache.spark.bigdl.api.python.BigDLSerDeBase$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(BigDLSerde.scala:83)
	at org.apache.spark.bigdl.api.python.BigDLSerDeBase$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(BigDLSerde.scala:82)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 13:29:17 WARN  TaskSetManager:66 - Lost task 7.0 in stage 0.0 (TID 7, localhost, executor driver): java.lang.ClassCastException: com.intel.analytics.bigdl.python.api.Sample cannot be cast to java.lang.String
	at org.apache.spark.bigdl.api.python.BigDLSerDe$SamplePickler.construct(BigDLSerde.scala:224)
	at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)
	at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)
	at net.razorvine.pickle.Unpickler.load(Unpickler.java:99)
	at net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)
	at org.apache.spark.bigdl.api.python.BigDLSerDeBase$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(BigDLSerde.scala:83)
	at org.apache.spark.bigdl.api.python.BigDLSerDeBase$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(BigDLSerde.scala:82)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:29:17 ERROR TaskSetManager:70 - Task 7 in stage 0.0 failed 1 times; aborting job
2019-07-01 13:29:48 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:29:48 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:29:49 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:29:49 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:29:49 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:29:49 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:29:49 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:29:49 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:29:49 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.04159887s
2019-07-01 13:29:49 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:29:49 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:29:49 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018544754s
2019-07-01 13:29:49 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:29:49 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 13:29:49 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:29:49 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 13:29:49 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 13:33:12 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:33:12 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:33:13 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:33:13 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:33:13 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:33:13 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:33:13 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:33:13 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:33:13 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.05120835s
2019-07-01 13:33:13 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:33:13 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:33:13 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018364741s
2019-07-01 13:33:13 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:33:13 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 13:33:13 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:33:13 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 13:33:13 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 13:33:26 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:33:27 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:33:27 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:33:27 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:33:27 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:33:27 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:33:27 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:33:27 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:33:27 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.039561889s
2019-07-01 13:33:27 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:33:27 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:33:27 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.016846576s
2019-07-01 13:33:27 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:33:27 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 13:33:27 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:33:27 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 13:33:27 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 13:33:49 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:33:49 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:33:50 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:33:50 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:33:50 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:33:50 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:33:50 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:33:50 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:33:50 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.039546262s
2019-07-01 13:33:50 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:33:50 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:33:50 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.016079025s
2019-07-01 13:33:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:33:50 INFO  DistriOptimizer$:408 - [Epoch 1 4/6][Iteration 1][Wall Clock 0.118142742s] Trained 4 records in 0.118142742 seconds. Throughput is 33.85735 records/second. Loss is 4.011905. Lineard226178e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:33:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:33:51 INFO  DistriOptimizer$:408 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.205004794s] Trained 4 records in 0.086862052 seconds. Throughput is 46.05003 records/second. Loss is 1.9524243. Lineard226178e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:33:51 INFO  DistriOptimizer$:452 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.205004794s] Epoch finished. Wall clock time is 217.916966 ms
2019-07-01 13:33:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:33:51 INFO  DistriOptimizer$:408 - [Epoch 2 4/6][Iteration 3][Wall Clock 0.283904087s] Trained 4 records in 0.065987121 seconds. Throughput is 60.617893 records/second. Loss is 2.0265586. Lineard226178e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:33:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:33:51 INFO  DistriOptimizer$:408 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.350557137s] Trained 4 records in 0.06665305 seconds. Throughput is 60.012257 records/second. Loss is 1.9020354. Lineard226178e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:33:51 INFO  DistriOptimizer$:452 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.350557137s] Epoch finished. Wall clock time is 364.014286 ms
2019-07-01 13:33:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:33:51 INFO  DistriOptimizer$:408 - [Epoch 3 4/6][Iteration 5][Wall Clock 0.437518027s] Trained 4 records in 0.073503741 seconds. Throughput is 54.419 records/second. Loss is 1.614747. Lineard226178e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:33:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:33:51 INFO  DistriOptimizer$:408 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.511370617s] Trained 4 records in 0.07385259 seconds. Throughput is 54.161945 records/second. Loss is 1.7925278. Lineard226178e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:33:51 INFO  DistriOptimizer$:452 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.511370617s] Epoch finished. Wall clock time is 531.900656 ms
2019-07-01 13:33:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:33:51 INFO  DistriOptimizer$:408 - [Epoch 4 4/6][Iteration 7][Wall Clock 0.630884091s] Trained 4 records in 0.098983435 seconds. Throughput is 40.4108 records/second. Loss is 1.5335131. Lineard226178e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:33:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:33:51 INFO  DistriOptimizer$:408 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.712646111s] Trained 4 records in 0.08176202 seconds. Throughput is 48.92247 records/second. Loss is 2.456071. Lineard226178e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:33:51 INFO  DistriOptimizer$:452 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.712646111s] Epoch finished. Wall clock time is 721.709612 ms
2019-07-01 13:33:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:33:51 INFO  DistriOptimizer$:408 - [Epoch 5 4/6][Iteration 9][Wall Clock 0.754180483s] Trained 4 records in 0.032470871 seconds. Throughput is 123.18733 records/second. Loss is 2.2485445. Lineard226178e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:33:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:33:51 INFO  DistriOptimizer$:408 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.790808208s] Trained 4 records in 0.036627725 seconds. Throughput is 109.206894 records/second. Loss is 1.1441039. Lineard226178e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:33:51 INFO  DistriOptimizer$:452 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.790808208s] Epoch finished. Wall clock time is 796.463361 ms
2019-07-01 13:34:45 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:34:45 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:34:46 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:34:46 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:34:46 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:34:46 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:34:46 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:34:46 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:34:46 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.045176255s
2019-07-01 13:34:46 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:34:46 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:34:46 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018737949s
2019-07-01 13:34:46 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 6 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:34:46 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.IllegalArgumentException: requirement failed: total batch size: 6 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 13:34:46 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 6 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:34:46 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 13:34:46 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 6 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 6 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 13:35:06 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:35:06 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:35:07 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:35:07 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:35:07 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:35:07 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:35:07 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:35:07 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:35:07 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.040770998s
2019-07-01 13:35:07 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:35:07 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:35:07 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018208575s
2019-07-01 13:35:07 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 5 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:35:07 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.IllegalArgumentException: requirement failed: total batch size: 5 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 13:35:07 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 5 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 13:35:07 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 13:35:07 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 5 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 5 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 13:35:18 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:35:18 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:35:19 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:35:19 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:35:19 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:35:19 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:35:19 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:35:19 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:35:19 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.043216224s
2019-07-01 13:35:19 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:35:19 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:35:19 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.020520618s
2019-07-01 13:35:19 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:35:19 INFO  DistriOptimizer$:408 - [Epoch 1 4/6][Iteration 1][Wall Clock 0.128046643s] Trained 4 records in 0.128046643 seconds. Throughput is 31.238615 records/second. Loss is 0.13049482. Linearb191d31a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:35:19 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:35:19 INFO  DistriOptimizer$:408 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.211527691s] Trained 4 records in 0.083481048 seconds. Throughput is 47.915066 records/second. Loss is 0.36772954. Linearb191d31a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:35:19 INFO  DistriOptimizer$:452 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.211527691s] Epoch finished. Wall clock time is 222.328516 ms
2019-07-01 13:35:19 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:35:19 INFO  DistriOptimizer$:408 - [Epoch 2 4/6][Iteration 3][Wall Clock 0.284760803s] Trained 4 records in 0.062432287 seconds. Throughput is 64.06941 records/second. Loss is 0.42445892. Linearb191d31a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:35:19 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:35:19 INFO  DistriOptimizer$:408 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.344836297s] Trained 4 records in 0.060075494 seconds. Throughput is 66.582886 records/second. Loss is 0.37437522. Linearb191d31a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:35:19 INFO  DistriOptimizer$:452 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.344836297s] Epoch finished. Wall clock time is 354.940883 ms
2019-07-01 13:35:19 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:35:19 INFO  DistriOptimizer$:408 - [Epoch 3 4/6][Iteration 5][Wall Clock 0.422160941s] Trained 4 records in 0.067220058 seconds. Throughput is 59.50605 records/second. Loss is 0.1867917. Linearb191d31a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:35:19 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:35:19 INFO  DistriOptimizer$:408 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.478933411s] Trained 4 records in 0.05677247 seconds. Throughput is 70.45668 records/second. Loss is 0.25290442. Linearb191d31a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:35:19 INFO  DistriOptimizer$:452 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.478933411s] Epoch finished. Wall clock time is 493.757636 ms
2019-07-01 13:35:19 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:35:19 INFO  DistriOptimizer$:408 - [Epoch 4 4/6][Iteration 7][Wall Clock 0.599576355s] Trained 4 records in 0.105818719 seconds. Throughput is 37.800495 records/second. Loss is 0.2742782. Linearb191d31a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:35:19 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:35:19 INFO  DistriOptimizer$:408 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.687905812s] Trained 4 records in 0.088329457 seconds. Throughput is 45.285007 records/second. Loss is 0.16647586. Linearb191d31a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:35:19 INFO  DistriOptimizer$:452 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.687905812s] Epoch finished. Wall clock time is 700.230297 ms
2019-07-01 13:35:20 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:35:20 INFO  DistriOptimizer$:408 - [Epoch 5 4/6][Iteration 9][Wall Clock 0.746312504s] Trained 4 records in 0.046082207 seconds. Throughput is 86.8014 records/second. Loss is 0.07708711. Linearb191d31a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:35:20 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:35:20 INFO  DistriOptimizer$:408 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.781297302s] Trained 4 records in 0.034984798 seconds. Throughput is 114.33538 records/second. Loss is 0.2135802. Linearb191d31a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:35:20 INFO  DistriOptimizer$:452 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.781297302s] Epoch finished. Wall clock time is 789.064045 ms
2019-07-01 13:46:03 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:46:03 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:46:04 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:46:04 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:46:04 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:46:04 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:46:04 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:46:04 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:46:04 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.044655162s
2019-07-01 13:46:04 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:46:04 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:46:04 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017211646s
2019-07-01 13:46:04 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:46:04 INFO  DistriOptimizer$:408 - [Epoch 1 4/6][Iteration 1][Wall Clock 0.116253607s] Trained 4 records in 0.116253607 seconds. Throughput is 34.407536 records/second. Loss is 8.283197. Linearf297808b's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:46:04 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:46:04 INFO  DistriOptimizer$:408 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.191032611s] Trained 4 records in 0.074779004 seconds. Throughput is 53.49095 records/second. Loss is 11.099682. Linearf297808b's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:46:04 INFO  DistriOptimizer$:452 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.191032611s] Epoch finished. Wall clock time is 203.738803 ms
2019-07-01 13:46:04 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:46:04 INFO  DistriOptimizer$:408 - [Epoch 2 4/6][Iteration 3][Wall Clock 0.269680399s] Trained 4 records in 0.065941596 seconds. Throughput is 60.65974 records/second. Loss is 14.0050125. Linearf297808b's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:46:04 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:46:04 INFO  DistriOptimizer$:408 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.337712642s] Trained 4 records in 0.068032243 seconds. Throughput is 58.79565 records/second. Loss is 6.8501463. Linearf297808b's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:46:04 INFO  DistriOptimizer$:452 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.337712642s] Epoch finished. Wall clock time is 347.886232 ms
2019-07-01 13:46:04 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:46:04 INFO  DistriOptimizer$:408 - [Epoch 3 4/6][Iteration 5][Wall Clock 0.419200414s] Trained 4 records in 0.071314182 seconds. Throughput is 56.08983 records/second. Loss is 5.6337614. Linearf297808b's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:46:04 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:46:04 INFO  DistriOptimizer$:408 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.486145372s] Trained 4 records in 0.066944958 seconds. Throughput is 59.75058 records/second. Loss is 13.436917. Linearf297808b's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:46:04 INFO  DistriOptimizer$:452 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.486145372s] Epoch finished. Wall clock time is 498.296167 ms
2019-07-01 13:46:04 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:46:04 INFO  DistriOptimizer$:408 - [Epoch 4 4/6][Iteration 7][Wall Clock 0.594792571s] Trained 4 records in 0.096496404 seconds. Throughput is 41.452324 records/second. Loss is 4.8052573. Linearf297808b's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:46:04 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:46:04 INFO  DistriOptimizer$:408 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.672140854s] Trained 4 records in 0.077348283 seconds. Throughput is 51.714138 records/second. Loss is 9.791361. Linearf297808b's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:46:04 INFO  DistriOptimizer$:452 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.672140854s] Epoch finished. Wall clock time is 688.217048 ms
2019-07-01 13:46:05 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:46:05 INFO  DistriOptimizer$:408 - [Epoch 5 4/6][Iteration 9][Wall Clock 0.724592415s] Trained 4 records in 0.036375367 seconds. Throughput is 109.96453 records/second. Loss is 10.889874. Linearf297808b's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:46:05 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:46:05 INFO  DistriOptimizer$:408 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.759933443s] Trained 4 records in 0.035341028 seconds. Throughput is 113.1829 records/second. Loss is 6.9813604. Linearf297808b's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:46:05 INFO  DistriOptimizer$:452 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.759933443s] Epoch finished. Wall clock time is 765.305408 ms
2019-07-01 13:47:48 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:48:36 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:48:37 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:48:37 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:48:37 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:48:37 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:48:37 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:48:37 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:48:37 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.041079227s
2019-07-01 13:48:37 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:48:37 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:48:37 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017122914s
2019-07-01 13:48:37 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:48:37 INFO  DistriOptimizer$:408 - [Epoch 1 4/6][Iteration 1][Wall Clock 0.112915531s] Trained 4 records in 0.112915531 seconds. Throughput is 35.42471 records/second. Loss is 9.663345. Linear6d6f480's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:48:37 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:48:37 INFO  DistriOptimizer$:408 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.177785547s] Trained 4 records in 0.064870016 seconds. Throughput is 61.66177 records/second. Loss is 5.5364337. Linear6d6f480's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:48:37 INFO  DistriOptimizer$:452 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.177785547s] Epoch finished. Wall clock time is 196.631985 ms
2019-07-01 13:48:37 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:48:37 INFO  DistriOptimizer$:408 - [Epoch 2 4/6][Iteration 3][Wall Clock 0.263035065s] Trained 4 records in 0.06640308 seconds. Throughput is 60.238167 records/second. Loss is 7.9874153. Linear6d6f480's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:48:37 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:48:38 INFO  DistriOptimizer$:408 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.326006458s] Trained 4 records in 0.062971393 seconds. Throughput is 63.52091 records/second. Loss is 6.0820637. Linear6d6f480's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:48:38 INFO  DistriOptimizer$:452 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.326006458s] Epoch finished. Wall clock time is 338.504827 ms
2019-07-01 13:48:38 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:48:38 INFO  DistriOptimizer$:408 - [Epoch 3 4/6][Iteration 5][Wall Clock 0.400152584s] Trained 4 records in 0.061647757 seconds. Throughput is 64.88476 records/second. Loss is 3.3349824. Linear6d6f480's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:48:38 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:48:38 INFO  DistriOptimizer$:408 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.506283735s] Trained 4 records in 0.106131151 seconds. Throughput is 37.689217 records/second. Loss is 3.9194887. Linear6d6f480's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:48:38 INFO  DistriOptimizer$:452 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.506283735s] Epoch finished. Wall clock time is 515.921842 ms
2019-07-01 13:48:38 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:48:38 INFO  DistriOptimizer$:408 - [Epoch 4 4/6][Iteration 7][Wall Clock 0.590841255s] Trained 4 records in 0.074919413 seconds. Throughput is 53.390705 records/second. Loss is 3.9416654. Linear6d6f480's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:48:38 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:48:38 INFO  DistriOptimizer$:408 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.64503983s] Trained 4 records in 0.054198575 seconds. Throughput is 73.80268 records/second. Loss is 3.641786. Linear6d6f480's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:48:38 INFO  DistriOptimizer$:452 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.64503983s] Epoch finished. Wall clock time is 665.765902 ms
2019-07-01 13:48:38 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:48:38 INFO  DistriOptimizer$:408 - [Epoch 5 4/6][Iteration 9][Wall Clock 0.715267115s] Trained 4 records in 0.049501213 seconds. Throughput is 80.8061 records/second. Loss is 2.7287. Linear6d6f480's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:48:38 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:48:38 INFO  DistriOptimizer$:408 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.746874582s] Trained 4 records in 0.031607467 seconds. Throughput is 126.55237 records/second. Loss is 2.5713942. Linear6d6f480's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:48:38 INFO  DistriOptimizer$:452 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.746874582s] Epoch finished. Wall clock time is 752.106243 ms
2019-07-01 13:51:45 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 13:52:11 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 13:52:11 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 13:52:11 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 13:52:11 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 13:52:11 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 13:52:11 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 13:52:11 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 13:52:11 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.038836069s
2019-07-01 13:52:11 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 13:52:11 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 13:52:11 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.015244247s
2019-07-01 13:52:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:52:12 INFO  DistriOptimizer$:408 - [Epoch 1 4/6][Iteration 1][Wall Clock 0.120024272s] Trained 4 records in 0.120024272 seconds. Throughput is 33.32659 records/second. Loss is 19.875013. Linear24af9fa6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:52:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:52:12 INFO  DistriOptimizer$:408 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.187528301s] Trained 4 records in 0.067504029 seconds. Throughput is 59.255726 records/second. Loss is 18.65202. Linear24af9fa6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:52:12 INFO  DistriOptimizer$:452 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.187528301s] Epoch finished. Wall clock time is 199.425271 ms
2019-07-01 13:52:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:52:12 INFO  DistriOptimizer$:408 - [Epoch 2 4/6][Iteration 3][Wall Clock 0.279889115s] Trained 4 records in 0.080463844 seconds. Throughput is 49.71177 records/second. Loss is 8.82968. Linear24af9fa6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:52:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:52:12 INFO  DistriOptimizer$:408 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.349057926s] Trained 4 records in 0.069168811 seconds. Throughput is 57.82953 records/second. Loss is 8.045553. Linear24af9fa6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:52:12 INFO  DistriOptimizer$:452 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.349057926s] Epoch finished. Wall clock time is 363.102186 ms
2019-07-01 13:52:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:52:12 INFO  DistriOptimizer$:408 - [Epoch 3 4/6][Iteration 5][Wall Clock 0.42532852s] Trained 4 records in 0.062226334 seconds. Throughput is 64.28147 records/second. Loss is 15.174184. Linear24af9fa6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:52:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:52:12 INFO  DistriOptimizer$:408 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.509891127s] Trained 4 records in 0.084562607 seconds. Throughput is 47.30223 records/second. Loss is 6.8994064. Linear24af9fa6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:52:12 INFO  DistriOptimizer$:452 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.509891127s] Epoch finished. Wall clock time is 519.383148 ms
2019-07-01 13:52:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:52:12 INFO  DistriOptimizer$:408 - [Epoch 4 4/6][Iteration 7][Wall Clock 0.635711078s] Trained 4 records in 0.11632793 seconds. Throughput is 34.38555 records/second. Loss is 11.461172. Linear24af9fa6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:52:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:52:12 INFO  DistriOptimizer$:408 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.692824817s] Trained 4 records in 0.057113739 seconds. Throughput is 70.03568 records/second. Loss is 11.029583. Linear24af9fa6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:52:12 INFO  DistriOptimizer$:452 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.692824817s] Epoch finished. Wall clock time is 710.449292 ms
2019-07-01 13:52:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:52:12 INFO  DistriOptimizer$:408 - [Epoch 5 4/6][Iteration 9][Wall Clock 0.750465642s] Trained 4 records in 0.04001635 seconds. Throughput is 99.959145 records/second. Loss is 7.09908. Linear24af9fa6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:52:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 13:52:12 INFO  DistriOptimizer$:408 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.785276779s] Trained 4 records in 0.034811137 seconds. Throughput is 114.90576 records/second. Loss is 4.01012. Linear24af9fa6's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 13:52:12 INFO  DistriOptimizer$:452 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.785276779s] Epoch finished. Wall clock time is 790.892155 ms
2019-07-01 14:10:02 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:10:15 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:10:15 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:10:15 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:10:15 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:10:15 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:10:15 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:10:15 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:10:15 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.038600824s
2019-07-01 14:10:15 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:10:15 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:10:15 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.015278945s
2019-07-01 14:10:15 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:10:16 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:10:16 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 14:10:16 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 14:11:19 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:11:35 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:11:36 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:11:36 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:11:36 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:11:36 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:11:36 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:11:36 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:11:36 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.03899379s
2019-07-01 14:11:36 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:11:36 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:11:36 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017841155s
2019-07-01 14:11:36 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:11:36 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:11:36 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:11:36 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 14:11:36 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 14:16:04 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:16:05 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:16:05 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:16:05 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:16:05 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:16:05 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:16:05 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:16:05 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:16:05 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.037888835s
2019-07-01 14:16:05 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:16:05 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:16:05 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.016879645s
2019-07-01 14:16:06 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:16:06 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:16:06 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:16:06 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 14:16:06 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 14:17:58 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:17:58 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:17:59 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:17:59 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:17:59 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:17:59 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:17:59 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:17:59 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:17:59 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.046877738s
2019-07-01 14:17:59 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:17:59 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:17:59 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.019782448s
2019-07-01 14:17:59 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:17:59 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:17:59 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:17:59 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 14:17:59 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 14:22:16 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:22:16 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:22:17 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:22:17 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:22:17 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:22:17 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:22:17 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:22:17 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:22:17 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.04118307s
2019-07-01 14:22:17 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:22:17 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:22:17 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018503461s
2019-07-01 14:22:17 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:22:17 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:22:17 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:22:17 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 14:22:17 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 14:23:00 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:23:00 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:23:01 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:23:01 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:23:01 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:23:01 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:23:01 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:23:01 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:23:01 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.041398672s
2019-07-01 14:23:01 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:23:01 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:23:01 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017748464s
2019-07-01 14:23:01 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:23:01 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:23:01 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:23:01 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 14:23:01 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 14:27:44 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:27:45 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:27:45 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:27:45 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:27:45 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:27:45 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:27:45 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:27:45 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:27:45 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.039480253s
2019-07-01 14:27:45 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:27:45 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:27:45 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017215836s
2019-07-01 14:27:45 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:27:45 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:27:45 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:27:45 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 14:27:45 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 14:28:18 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:28:27 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:28:27 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:28:28 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:28:28 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:28:28 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:28:28 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:28:28 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:28:28 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:28:28 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.040408358s
2019-07-01 14:28:28 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:28:28 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:28:28 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017723231s
2019-07-01 14:28:28 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:28:28 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:28:28 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 14:28:28 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: setValue: samples's size doesn't match mini batch size, excepted 1 got 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:182)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:223)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 14:28:56 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:28:56 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:28:57 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:28:57 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:28:57 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:28:57 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:28:57 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:28:57 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:28:57 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.043545368s
2019-07-01 14:28:57 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:28:57 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:28:57 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017722751s
2019-07-01 14:28:57 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:28:57 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:28:57 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:28:57 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-01 14:28:57 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 14:48:37 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:48:37 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:48:38 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:48:38 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:48:38 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:48:38 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:48:38 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:48:38 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:48:38 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.03548591s
2019-07-01 14:48:38 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:48:38 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:48:38 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.022909797s
2019-07-01 14:48:38 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:48:38 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 20)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:48:38 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:48:38 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job
2019-07-01 14:48:38 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 14:53:49 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:53:49 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:53:50 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:53:50 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:53:50 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:53:50 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:53:50 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:53:50 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:53:50 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.038872652s
2019-07-01 14:53:50 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:53:50 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:53:50 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.01630023s
2019-07-01 14:53:50 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:53:50 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 20)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:53:50 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:53:50 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job
2019-07-01 14:53:50 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 14:57:45 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:57:45 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:57:46 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:57:46 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:57:46 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 14:57:46 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 14:57:46 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:57:46 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:57:46 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.040143841s
2019-07-01 14:57:46 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:57:46 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:57:46 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017149883s
2019-07-01 14:57:46 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 14:57:46 INFO  DistriOptimizer$:408 - [Epoch 1 4/6][Iteration 1][Wall Clock 0.135655646s] Trained 4 records in 0.135655646 seconds. Throughput is 29.486425 records/second. Loss is 11.423924. Lineard254543a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 14:57:46 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 14:57:46 INFO  DistriOptimizer$:408 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.223418669s] Trained 4 records in 0.087763023 seconds. Throughput is 45.57728 records/second. Loss is 5.0075407. Lineard254543a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 14:57:46 INFO  DistriOptimizer$:452 - [Epoch 1 8/6][Iteration 2][Wall Clock 0.223418669s] Epoch finished. Wall clock time is 246.060899 ms
2019-07-01 14:57:46 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 14:57:46 INFO  DistriOptimizer$:408 - [Epoch 2 4/6][Iteration 3][Wall Clock 0.311683206s] Trained 4 records in 0.065622307 seconds. Throughput is 60.954884 records/second. Loss is 5.531325. Lineard254543a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 14:57:46 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 14:57:46 INFO  DistriOptimizer$:408 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.412817144s] Trained 4 records in 0.101133938 seconds. Throughput is 39.55151 records/second. Loss is 8.690717. Lineard254543a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 14:57:46 INFO  DistriOptimizer$:452 - [Epoch 2 8/6][Iteration 4][Wall Clock 0.412817144s] Epoch finished. Wall clock time is 431.654178 ms
2019-07-01 14:57:46 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 14:57:46 INFO  DistriOptimizer$:408 - [Epoch 3 4/6][Iteration 5][Wall Clock 0.507775786s] Trained 4 records in 0.076121608 seconds. Throughput is 52.5475 records/second. Loss is 5.8557296. Lineard254543a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 14:57:46 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 14:57:46 INFO  DistriOptimizer$:408 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.597900824s] Trained 4 records in 0.090125038 seconds. Throughput is 44.382782 records/second. Loss is 6.749873. Lineard254543a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 14:57:46 INFO  DistriOptimizer$:452 - [Epoch 3 8/6][Iteration 6][Wall Clock 0.597900824s] Epoch finished. Wall clock time is 614.985018 ms
2019-07-01 14:57:46 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 14:57:46 INFO  DistriOptimizer$:408 - [Epoch 4 4/6][Iteration 7][Wall Clock 0.724523549s] Trained 4 records in 0.109538531 seconds. Throughput is 36.51683 records/second. Loss is 8.482081. Lineard254543a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 14:57:46 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 14:57:46 INFO  DistriOptimizer$:408 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.785518911s] Trained 4 records in 0.060995362 seconds. Throughput is 65.57876 records/second. Loss is 6.9871273. Lineard254543a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 14:57:46 INFO  DistriOptimizer$:452 - [Epoch 4 8/6][Iteration 8][Wall Clock 0.785518911s] Epoch finished. Wall clock time is 804.390147 ms
2019-07-01 14:57:46 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 14:57:47 INFO  DistriOptimizer$:408 - [Epoch 5 4/6][Iteration 9][Wall Clock 0.852517108s] Trained 4 records in 0.048126961 seconds. Throughput is 83.113495 records/second. Loss is 6.8650894. Lineard254543a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 14:57:47 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 14:57:47 INFO  DistriOptimizer$:408 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.929230085s] Trained 4 records in 0.076712977 seconds. Throughput is 52.142418 records/second. Loss is 4.6705484. Lineard254543a's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 14:57:47 INFO  DistriOptimizer$:452 - [Epoch 5 8/6][Iteration 10][Wall Clock 0.929230085s] Epoch finished. Wall clock time is 938.156509 ms
2019-07-01 14:57:47 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 14:57:47 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 14:57:47 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 14:57:47 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 14:57:47 WARN  BlockManager:66 - Asked to remove block test_1weights0, which does not exist
2019-07-01 14:57:47 WARN  BlockManager:66 - Asked to remove block test_1gradients0, which does not exist
2019-07-01 14:57:47 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 14:57:47 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 14:57:47 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.03963021s
2019-07-01 14:57:47 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 14:57:47 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 14:57:47 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.002005241s
2019-07-01 14:57:47 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:57:47 ERROR Executor:91 - Exception in task 0.0 in stage 67.0 (TID 64)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-01 14:57:47 WARN  TaskSetManager:66 - Lost task 0.0 in stage 67.0 (TID 64, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 14:57:47 ERROR TaskSetManager:70 - Task 0 in stage 67.0 failed 1 times; aborting job
2019-07-01 14:57:47 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 67.0 failed 1 times, most recent failure: Lost task 0.0 in stage 67.0 (TID 64, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-01 15:24:42 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 15:25:13 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 15:25:14 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 15:25:14 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 15:25:14 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 15:25:14 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 15:25:14 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 15:25:14 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 15:25:14 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.036207108s
2019-07-01 15:25:14 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 15:25:14 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 15:25:14 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017099514s
2019-07-01 15:25:14 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:25:14 ERROR ThreadPool$:147 - Error: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:25:14 ERROR ThreadPool$:147 - Error: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:25:14 ERROR ThreadPool$:147 - Error: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:25:14 ERROR ThreadPool$:147 - Error: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:25:14 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 20)
java.util.concurrent.ExecutionException: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-01 15:25:14 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 15:25:14 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job
2019-07-01 15:25:14 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Linear[8f3bdfd3](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 15:30:27 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 15:30:27 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 15:30:27 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 15:30:27 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 15:30:27 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 15:30:27 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 15:30:27 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 15:30:27 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 15:30:28 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.041952992s
2019-07-01 15:30:28 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 15:30:28 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 15:30:28 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018930415s
2019-07-01 15:30:28 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:30:28 ERROR ThreadPool$:147 - Error: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:30:28 ERROR ThreadPool$:147 - Error: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:30:28 ERROR ThreadPool$:147 - Error: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:30:28 ERROR ThreadPool$:147 - Error: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:30:28 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 20)
java.util.concurrent.ExecutionException: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-01 15:30:28 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 15:30:28 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job
2019-07-01 15:30:28 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Linear[b0b84a3c](3 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 15:32:15 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 15:34:46 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 15:44:50 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 15:49:41 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 15:50:20 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 15:50:20 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 15:50:20 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 15:50:20 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 15:50:20 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 15:50:20 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 15:50:20 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 15:50:20 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 15:50:20 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.037117986s
2019-07-01 15:50:20 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 15:50:20 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 15:50:20 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017204754s
2019-07-01 15:50:20 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:50:20 ERROR ThreadPool$:147 - Error: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:50:20 ERROR ThreadPool$:147 - Error: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:50:20 ERROR ThreadPool$:147 - Error: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:50:20 ERROR ThreadPool$:147 - Error: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:50:20 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 20)
java.util.concurrent.ExecutionException: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-01 15:50:20 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 15:50:20 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job
2019-07-01 15:50:20 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Linear[3815b3cc](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 15:53:25 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 15:53:26 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 15:53:26 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 15:53:26 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 15:53:26 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 15:53:26 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 15:53:26 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 15:53:26 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 15:53:26 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.040815801s
2019-07-01 15:53:26 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 15:53:26 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 15:53:26 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018493006s
2019-07-01 15:53:26 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:53:26 ERROR ThreadPool$:147 - Error: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:53:26 ERROR ThreadPool$:147 - Error: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:53:26 ERROR ThreadPool$:147 - Error: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:53:26 ERROR ThreadPool$:147 - Error: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 15:53:26 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 20)
java.util.concurrent.ExecutionException: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-01 15:53:26 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 15:53:26 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job
2019-07-01 15:53:26 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Linear[e4dbd604](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 15:57:15 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 15:57:15 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 15:57:15 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 15:57:15 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 15:57:15 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 15:57:15 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 15:57:15 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 15:57:15 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 15:57:15 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.038773378s
2019-07-01 15:57:15 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 15:57:15 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 15:57:15 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018410004s
2019-07-01 15:57:15 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:57:16 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.133081441s] Trained 4 records in 0.133081441 seconds. Throughput is 30.056784 records/second. Loss is 0.049173478. Linear7bb18903's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 15:57:16 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.133081441s] Epoch finished. Wall clock time is 145.489987 ms
2019-07-01 15:57:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:57:16 INFO  DistriOptimizer$:408 - [Epoch 2 4/4][Iteration 2][Wall Clock 0.226128806s] Trained 4 records in 0.080638819 seconds. Throughput is 49.6039 records/second. Loss is 0.049066845. Linear7bb18903's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 15:57:16 INFO  DistriOptimizer$:452 - [Epoch 2 4/4][Iteration 2][Wall Clock 0.226128806s] Epoch finished. Wall clock time is 237.783974 ms
2019-07-01 15:57:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:57:16 INFO  DistriOptimizer$:408 - [Epoch 3 4/4][Iteration 3][Wall Clock 0.312457184s] Trained 4 records in 0.07467321 seconds. Throughput is 53.56673 records/second. Loss is 0.048554827. Linear7bb18903's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 15:57:16 INFO  DistriOptimizer$:452 - [Epoch 3 4/4][Iteration 3][Wall Clock 0.312457184s] Epoch finished. Wall clock time is 325.295319 ms
2019-07-01 15:57:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:57:16 INFO  DistriOptimizer$:408 - [Epoch 4 4/4][Iteration 4][Wall Clock 0.378792636s] Trained 4 records in 0.053497317 seconds. Throughput is 74.7701 records/second. Loss is 0.047673747. Linear7bb18903's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 15:57:16 INFO  DistriOptimizer$:452 - [Epoch 4 4/4][Iteration 4][Wall Clock 0.378792636s] Epoch finished. Wall clock time is 386.727892 ms
2019-07-01 15:57:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:57:16 INFO  DistriOptimizer$:408 - [Epoch 5 4/4][Iteration 5][Wall Clock 0.459406812s] Trained 4 records in 0.07267892 seconds. Throughput is 55.036587 records/second. Loss is 0.047673747. Linear7bb18903's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 15:57:16 INFO  DistriOptimizer$:452 - [Epoch 5 4/4][Iteration 5][Wall Clock 0.459406812s] Epoch finished. Wall clock time is 472.618388 ms
2019-07-01 15:57:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:57:16 INFO  DistriOptimizer$:408 - [Epoch 6 4/4][Iteration 6][Wall Clock 0.56638714s] Trained 4 records in 0.093768752 seconds. Throughput is 42.658134 records/second. Loss is 0.04756889. Linear7bb18903's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 15:57:16 INFO  DistriOptimizer$:452 - [Epoch 6 4/4][Iteration 6][Wall Clock 0.56638714s] Epoch finished. Wall clock time is 575.019261 ms
2019-07-01 15:57:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:57:16 INFO  DistriOptimizer$:408 - [Epoch 7 4/4][Iteration 7][Wall Clock 0.650821559s] Trained 4 records in 0.075802298 seconds. Throughput is 52.76885 records/second. Loss is 0.04756889. Linear7bb18903's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 15:57:16 INFO  DistriOptimizer$:452 - [Epoch 7 4/4][Iteration 7][Wall Clock 0.650821559s] Epoch finished. Wall clock time is 667.213452 ms
2019-07-01 15:57:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:57:16 INFO  DistriOptimizer$:408 - [Epoch 8 4/4][Iteration 8][Wall Clock 0.727323331s] Trained 4 records in 0.060109879 seconds. Throughput is 66.5448 records/second. Loss is 0.04756889. Linear7bb18903's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 15:57:16 INFO  DistriOptimizer$:452 - [Epoch 8 4/4][Iteration 8][Wall Clock 0.727323331s] Epoch finished. Wall clock time is 734.630763 ms
2019-07-01 15:57:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:57:16 INFO  DistriOptimizer$:408 - [Epoch 9 4/4][Iteration 9][Wall Clock 0.776171714s] Trained 4 records in 0.041540951 seconds. Throughput is 96.29053 records/second. Loss is 0.046698406. Linear7bb18903's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 15:57:16 INFO  DistriOptimizer$:452 - [Epoch 9 4/4][Iteration 9][Wall Clock 0.776171714s] Epoch finished. Wall clock time is 781.669506 ms
2019-07-01 15:57:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 15:57:16 INFO  DistriOptimizer$:408 - [Epoch 10 4/4][Iteration 10][Wall Clock 0.816178995s] Trained 4 records in 0.034509489 seconds. Throughput is 115.91016 records/second. Loss is 0.046698406. Linear7bb18903's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 15:57:16 INFO  DistriOptimizer$:452 - [Epoch 10 4/4][Iteration 10][Wall Clock 0.816178995s] Epoch finished. Wall clock time is 821.255436 ms
2019-07-01 16:02:29 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 16:02:29 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 16:02:29 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 16:02:29 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 16:02:29 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 16:02:29 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 16:02:29 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 16:02:29 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 16:02:29 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.044605481s
2019-07-01 16:02:29 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 16:02:29 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 16:02:29 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.026207373s
2019-07-01 16:02:29 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:02:29 ERROR ThreadPool$:147 - Error: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:02:29 ERROR ThreadPool$:147 - Error: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:02:29 ERROR ThreadPool$:147 - Error: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:02:29 ERROR ThreadPool$:147 - Error: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:02:29 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 20)
java.util.concurrent.ExecutionException: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-01 16:02:29 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 16:02:29 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job
2019-07-01 16:02:29 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Linear[d60f706e](2 -> 1)
java.lang.IllegalArgumentException: requirement failed: size mismatch, m1:1x4 m2:2x1
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.addmm(DenseTensorMath.scala:517)
	at com.intel.analytics.bigdl.tensor.DenseTensor.addmm(DenseTensor.scala:1259)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:108)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 16:02:49 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 16:02:49 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 16:02:49 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 16:02:49 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 16:02:49 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 16:02:49 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 16:02:49 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 16:02:49 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 16:02:50 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.03962216s
2019-07-01 16:02:50 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 16:02:50 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 16:02:50 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.02420309s
2019-07-01 16:02:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:02:50 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.118207947s] Trained 4 records in 0.118207947 seconds. Throughput is 33.838673 records/second. Loss is 0.83558935. Linear2724cf24's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:02:50 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.118207947s] Epoch finished. Wall clock time is 129.773578 ms
2019-07-01 16:02:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:02:50 INFO  DistriOptimizer$:408 - [Epoch 2 4/4][Iteration 2][Wall Clock 0.211099285s] Trained 4 records in 0.081325707 seconds. Throughput is 49.184937 records/second. Loss is 0.8288903. Linear2724cf24's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:02:50 INFO  DistriOptimizer$:452 - [Epoch 2 4/4][Iteration 2][Wall Clock 0.211099285s] Epoch finished. Wall clock time is 223.287286 ms
2019-07-01 16:02:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:02:50 INFO  DistriOptimizer$:408 - [Epoch 3 4/4][Iteration 3][Wall Clock 0.297573515s] Trained 4 records in 0.074286229 seconds. Throughput is 53.84578 records/second. Loss is 0.8222507. Linear2724cf24's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:02:50 INFO  DistriOptimizer$:452 - [Epoch 3 4/4][Iteration 3][Wall Clock 0.297573515s] Epoch finished. Wall clock time is 311.289402 ms
2019-07-01 16:02:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:02:50 INFO  DistriOptimizer$:408 - [Epoch 4 4/4][Iteration 4][Wall Clock 0.38354255s] Trained 4 records in 0.072253148 seconds. Throughput is 55.360912 records/second. Loss is 0.81560695. Linear2724cf24's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:02:50 INFO  DistriOptimizer$:452 - [Epoch 4 4/4][Iteration 4][Wall Clock 0.38354255s] Epoch finished. Wall clock time is 391.74077 ms
2019-07-01 16:02:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:02:50 INFO  DistriOptimizer$:408 - [Epoch 5 4/4][Iteration 5][Wall Clock 0.475543635s] Trained 4 records in 0.083802865 seconds. Throughput is 47.731064 records/second. Loss is 0.80958766. Linear2724cf24's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:02:50 INFO  DistriOptimizer$:452 - [Epoch 5 4/4][Iteration 5][Wall Clock 0.475543635s] Epoch finished. Wall clock time is 483.731463 ms
2019-07-01 16:02:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:02:50 INFO  DistriOptimizer$:408 - [Epoch 6 4/4][Iteration 6][Wall Clock 0.576512798s] Trained 4 records in 0.092781335 seconds. Throughput is 43.112118 records/second. Loss is 0.8002519. Linear2724cf24's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:02:50 INFO  DistriOptimizer$:452 - [Epoch 6 4/4][Iteration 6][Wall Clock 0.576512798s] Epoch finished. Wall clock time is 594.516892 ms
2019-07-01 16:02:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:02:50 INFO  DistriOptimizer$:408 - [Epoch 7 4/4][Iteration 7][Wall Clock 0.661697818s] Trained 4 records in 0.067180926 seconds. Throughput is 59.54071 records/second. Loss is 0.79653955. Linear2724cf24's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:02:50 INFO  DistriOptimizer$:452 - [Epoch 7 4/4][Iteration 7][Wall Clock 0.661697818s] Epoch finished. Wall clock time is 669.234834 ms
2019-07-01 16:02:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:02:50 INFO  DistriOptimizer$:408 - [Epoch 8 4/4][Iteration 8][Wall Clock 0.704997777s] Trained 4 records in 0.035762943 seconds. Throughput is 111.84762 records/second. Loss is 0.7872052. Linear2724cf24's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:02:50 INFO  DistriOptimizer$:452 - [Epoch 8 4/4][Iteration 8][Wall Clock 0.704997777s] Epoch finished. Wall clock time is 709.713042 ms
2019-07-01 16:02:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:02:50 INFO  DistriOptimizer$:408 - [Epoch 9 4/4][Iteration 9][Wall Clock 0.746705177s] Trained 4 records in 0.036992135 seconds. Throughput is 108.13109 records/second. Loss is 0.78078324. Linear2724cf24's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:02:50 INFO  DistriOptimizer$:452 - [Epoch 9 4/4][Iteration 9][Wall Clock 0.746705177s] Epoch finished. Wall clock time is 751.771809 ms
2019-07-01 16:02:50 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:02:50 INFO  DistriOptimizer$:408 - [Epoch 10 4/4][Iteration 10][Wall Clock 0.784625954s] Trained 4 records in 0.032854145 seconds. Throughput is 121.750244 records/second. Loss is 0.77426815. Linear2724cf24's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:02:50 INFO  DistriOptimizer$:452 - [Epoch 10 4/4][Iteration 10][Wall Clock 0.784625954s] Epoch finished. Wall clock time is 789.018844 ms
2019-07-01 16:04:06 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 16:04:06 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 16:04:06 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 16:04:06 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 16:04:06 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 16:04:06 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 16:04:06 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 16:04:06 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 16:04:06 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.0404772s
2019-07-01 16:04:06 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 16:04:06 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 16:04:06 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.02086529s
2019-07-01 16:04:06 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:04:06 ERROR ThreadPool$:147 - Error: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:04:06 ERROR ThreadPool$:147 - Error: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:04:06 ERROR ThreadPool$:147 - Error: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:04:06 ERROR ThreadPool$:147 - Error: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:04:06 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 20)
java.util.concurrent.ExecutionException: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-01 16:04:06 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 16:04:06 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job
2019-07-01 16:04:06 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Linear[7f35835b](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 16:04:27 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 16:04:27 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 16:04:28 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 16:04:28 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 16:04:28 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 16:04:28 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 16:04:28 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 16:04:28 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 16:04:28 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.039726429s
2019-07-01 16:04:28 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 16:04:28 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 16:04:28 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.015680768s
2019-07-01 16:04:28 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:04:28 ERROR ThreadPool$:147 - Error: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:04:28 ERROR ThreadPool$:147 - Error: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:04:28 ERROR ThreadPool$:147 - Error: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:04:28 ERROR ThreadPool$:147 - Error: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:04:28 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 20)
java.util.concurrent.ExecutionException: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-01 16:04:28 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 16:04:28 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job
2019-07-01 16:04:28 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Linear[ee309ba](1 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 16:08:39 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 16:08:39 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 16:08:40 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 16:08:40 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 16:08:40 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 16:08:40 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 16:08:40 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 16:08:40 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 16:08:40 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.042003049s
2019-07-01 16:08:40 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 16:08:40 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 16:08:40 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.021657435s
2019-07-01 16:08:40 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:08:40 ERROR ThreadPool$:147 - Error: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:08:40 ERROR ThreadPool$:147 - Error: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:08:40 ERROR ThreadPool$:147 - Error: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:08:40 ERROR ThreadPool$:147 - Error: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:08:40 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 20)
java.util.concurrent.ExecutionException: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-01 16:08:40 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 16:08:40 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job
2019-07-01 16:08:40 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Linear[31063c21](4 -> 1)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 16:09:10 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 16:09:10 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 16:09:10 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 16:09:11 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 16:09:11 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 16:09:11 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 16:09:11 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 16:09:11 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 16:09:11 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.039701474s
2019-07-01 16:09:11 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 16:09:11 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 16:09:11 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.021793477s
2019-07-01 16:09:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:11 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.1228227s] Trained 4 records in 0.1228227 seconds. Throughput is 32.56727 records/second. Loss is 1.3854152. Linearb656a701's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:11 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.1228227s] Epoch finished. Wall clock time is 139.709404 ms
2019-07-01 16:09:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:11 INFO  DistriOptimizer$:408 - [Epoch 2 4/4][Iteration 2][Wall Clock 0.22350625s] Trained 4 records in 0.083796846 seconds. Throughput is 47.734493 records/second. Loss is 1.3640155. Linearb656a701's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:11 INFO  DistriOptimizer$:452 - [Epoch 2 4/4][Iteration 2][Wall Clock 0.22350625s] Epoch finished. Wall clock time is 233.750717 ms
2019-07-01 16:09:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:11 INFO  DistriOptimizer$:408 - [Epoch 3 4/4][Iteration 3][Wall Clock 0.31514152s] Trained 4 records in 0.081390803 seconds. Throughput is 49.1456 records/second. Loss is 1.3427982. Linearb656a701's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:11 INFO  DistriOptimizer$:452 - [Epoch 3 4/4][Iteration 3][Wall Clock 0.31514152s] Epoch finished. Wall clock time is 326.957468 ms
2019-07-01 16:09:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:11 INFO  DistriOptimizer$:408 - [Epoch 4 4/4][Iteration 4][Wall Clock 0.409211316s] Trained 4 records in 0.082253848 seconds. Throughput is 48.629944 records/second. Loss is 1.324672. Linearb656a701's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:11 INFO  DistriOptimizer$:452 - [Epoch 4 4/4][Iteration 4][Wall Clock 0.409211316s] Epoch finished. Wall clock time is 422.001291 ms
2019-07-01 16:09:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:11 INFO  DistriOptimizer$:408 - [Epoch 5 4/4][Iteration 5][Wall Clock 0.501612963s] Trained 4 records in 0.079611672 seconds. Throughput is 50.24389 records/second. Loss is 1.3049768. Linearb656a701's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:11 INFO  DistriOptimizer$:452 - [Epoch 5 4/4][Iteration 5][Wall Clock 0.501612963s] Epoch finished. Wall clock time is 513.128904 ms
2019-07-01 16:09:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:11 INFO  DistriOptimizer$:408 - [Epoch 6 4/4][Iteration 6][Wall Clock 0.616551373s] Trained 4 records in 0.103422469 seconds. Throughput is 38.676315 records/second. Loss is 1.2867811. Linearb656a701's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:11 INFO  DistriOptimizer$:452 - [Epoch 6 4/4][Iteration 6][Wall Clock 0.616551373s] Epoch finished. Wall clock time is 634.263923 ms
2019-07-01 16:09:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:11 INFO  DistriOptimizer$:408 - [Epoch 7 4/4][Iteration 7][Wall Clock 0.701334355s] Trained 4 records in 0.067070432 seconds. Throughput is 59.638798 records/second. Loss is 1.2665352. Linearb656a701's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:11 INFO  DistriOptimizer$:452 - [Epoch 7 4/4][Iteration 7][Wall Clock 0.701334355s] Epoch finished. Wall clock time is 709.528898 ms
2019-07-01 16:09:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:11 INFO  DistriOptimizer$:408 - [Epoch 8 4/4][Iteration 8][Wall Clock 0.749740811s] Trained 4 records in 0.040211913 seconds. Throughput is 99.473015 records/second. Loss is 1.2516116. Linearb656a701's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:11 INFO  DistriOptimizer$:452 - [Epoch 8 4/4][Iteration 8][Wall Clock 0.749740811s] Epoch finished. Wall clock time is 756.030617 ms
2019-07-01 16:09:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:11 INFO  DistriOptimizer$:408 - [Epoch 9 4/4][Iteration 9][Wall Clock 0.794038334s] Trained 4 records in 0.038007717 seconds. Throughput is 105.24178 records/second. Loss is 1.2303381. Linearb656a701's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:11 INFO  DistriOptimizer$:452 - [Epoch 9 4/4][Iteration 9][Wall Clock 0.794038334s] Epoch finished. Wall clock time is 799.614922 ms
2019-07-01 16:09:11 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:11 INFO  DistriOptimizer$:408 - [Epoch 10 4/4][Iteration 10][Wall Clock 0.838225292s] Trained 4 records in 0.03861037 seconds. Throughput is 103.59911 records/second. Loss is 1.213005. Linearb656a701's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:11 INFO  DistriOptimizer$:452 - [Epoch 10 4/4][Iteration 10][Wall Clock 0.838225292s] Epoch finished. Wall clock time is 844.058472 ms
2019-07-01 16:09:56 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 16:09:57 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 16:09:57 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 16:09:57 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 16:09:57 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 16:09:57 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 16:09:57 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 16:09:57 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 16:09:57 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.039939342s
2019-07-01 16:09:57 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 16:09:57 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 16:09:57 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.023108014s
2019-07-01 16:09:57 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:57 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.1401484s] Trained 4 records in 0.1401484 seconds. Throughput is 28.541174 records/second. Loss is 0.58710873. Linearea45419e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:57 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.1401484s] Epoch finished. Wall clock time is 160.434468 ms
2019-07-01 16:09:57 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:57 INFO  DistriOptimizer$:408 - [Epoch 2 4/4][Iteration 2][Wall Clock 0.238743444s] Trained 4 records in 0.078308976 seconds. Throughput is 51.079712 records/second. Loss is 0.5811372. Linearea45419e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:57 INFO  DistriOptimizer$:452 - [Epoch 2 4/4][Iteration 2][Wall Clock 0.238743444s] Epoch finished. Wall clock time is 254.486844 ms
2019-07-01 16:09:57 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:57 INFO  DistriOptimizer$:408 - [Epoch 3 4/4][Iteration 3][Wall Clock 0.327869055s] Trained 4 records in 0.073382211 seconds. Throughput is 54.50912 records/second. Loss is 0.57461923. Linearea45419e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:57 INFO  DistriOptimizer$:452 - [Epoch 3 4/4][Iteration 3][Wall Clock 0.327869055s] Epoch finished. Wall clock time is 336.904332 ms
2019-07-01 16:09:57 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:57 INFO  DistriOptimizer$:408 - [Epoch 4 4/4][Iteration 4][Wall Clock 0.419947499s] Trained 4 records in 0.083043167 seconds. Throughput is 48.16772 records/second. Loss is 0.56808865. Linearea45419e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:57 INFO  DistriOptimizer$:452 - [Epoch 4 4/4][Iteration 4][Wall Clock 0.419947499s] Epoch finished. Wall clock time is 428.321078 ms
2019-07-01 16:09:57 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:57 INFO  DistriOptimizer$:408 - [Epoch 5 4/4][Iteration 5][Wall Clock 0.516215189s] Trained 4 records in 0.087894111 seconds. Throughput is 45.509304 records/second. Loss is 0.55979717. Linearea45419e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:57 INFO  DistriOptimizer$:452 - [Epoch 5 4/4][Iteration 5][Wall Clock 0.516215189s] Epoch finished. Wall clock time is 526.229241 ms
2019-07-01 16:09:57 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:58 INFO  DistriOptimizer$:408 - [Epoch 6 4/4][Iteration 6][Wall Clock 0.628496062s] Trained 4 records in 0.102266821 seconds. Throughput is 39.113373 records/second. Loss is 0.5539739. Linearea45419e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:58 INFO  DistriOptimizer$:452 - [Epoch 6 4/4][Iteration 6][Wall Clock 0.628496062s] Epoch finished. Wall clock time is 643.536015 ms
2019-07-01 16:09:58 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:58 INFO  DistriOptimizer$:408 - [Epoch 7 4/4][Iteration 7][Wall Clock 0.695573758s] Trained 4 records in 0.052037743 seconds. Throughput is 76.86729 records/second. Loss is 0.5461757. Linearea45419e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:58 INFO  DistriOptimizer$:452 - [Epoch 7 4/4][Iteration 7][Wall Clock 0.695573758s] Epoch finished. Wall clock time is 702.14057 ms
2019-07-01 16:09:58 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:58 INFO  DistriOptimizer$:408 - [Epoch 8 4/4][Iteration 8][Wall Clock 0.74374942s] Trained 4 records in 0.04160885 seconds. Throughput is 96.13339 records/second. Loss is 0.54031754. Linearea45419e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:58 INFO  DistriOptimizer$:452 - [Epoch 8 4/4][Iteration 8][Wall Clock 0.74374942s] Epoch finished. Wall clock time is 749.729616 ms
2019-07-01 16:09:58 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:58 INFO  DistriOptimizer$:408 - [Epoch 9 4/4][Iteration 9][Wall Clock 0.784890263s] Trained 4 records in 0.035160647 seconds. Throughput is 113.76355 records/second. Loss is 0.5332247. Linearea45419e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:58 INFO  DistriOptimizer$:452 - [Epoch 9 4/4][Iteration 9][Wall Clock 0.784890263s] Epoch finished. Wall clock time is 790.695831 ms
2019-07-01 16:09:58 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:09:58 INFO  DistriOptimizer$:408 - [Epoch 10 4/4][Iteration 10][Wall Clock 0.824725844s] Trained 4 records in 0.034030013 seconds. Throughput is 117.5433 records/second. Loss is 0.5269532. Linearea45419e's hyper parameters: Current learning rate is 0.001. Current dampening is 1.7976931348623157E308.  
2019-07-01 16:09:58 INFO  DistriOptimizer$:452 - [Epoch 10 4/4][Iteration 10][Wall Clock 0.824725844s] Epoch finished. Wall clock time is 829.799691 ms
2019-07-01 16:16:25 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 16:17:39 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-07-01 16:17:40 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-01 16:17:40 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-01 16:17:40 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-01 16:17:40 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-01 16:17:40 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-01 16:17:40 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-01 16:17:40 INFO  DistriOptimizer$:154 - Count dataset
2019-07-01 16:17:40 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.047319109s
2019-07-01 16:17:40 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-01 16:17:40 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-01 16:17:40 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.020242975s
2019-07-01 16:17:40 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-01 16:17:40 ERROR ThreadPool$:147 - Error: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:17:40 ERROR ThreadPool$:147 - Error: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:17:40 ERROR ThreadPool$:147 - Error: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:17:40 ERROR ThreadPool$:147 - Error: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-01 16:17:40 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 20)
java.util.concurrent.ExecutionException: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-01 16:17:40 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 16:17:40 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job
2019-07-01 16:17:40 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 20, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Linear[c2f38eb0](2 -> 4)
java.lang.IllegalArgumentException: requirement failed: Linear: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim 3
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:85)
	at com.intel.analytics.bigdl.nn.Linear.updateOutput(Linear.scala:44)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-01 19:20:08 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:20:08 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:20:08 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:20:08 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)

2019-07-01 19:20:08 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-07-01 19:21:56 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:21:56 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:21:56 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:21:56 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:21:56 ERROR Executor:91 - Exception in task 2.0 in stage 1.0 (TID 3)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:21:56 ERROR Executor:91 - Exception in task 1.0 in stage 1.0 (TID 2)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:21:56 WARN  TaskSetManager:66 - Lost task 2.0 in stage 1.0 (TID 3, localhost, executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)

2019-07-01 19:21:56 ERROR TaskSetManager:70 - Task 2 in stage 1.0 failed 1 times; aborting job
2019-07-01 19:21:56 WARN  TaskSetManager:66 - Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): TaskKilled (Stage cancelled)
2019-07-01 19:21:56 WARN  TaskSetManager:66 - Lost task 3.0 in stage 1.0 (TID 4, localhost, executor driver): TaskKilled (Stage cancelled)
2019-07-01 19:22:28 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:22:28 ERROR Executor:91 - Exception in task 2.0 in stage 3.0 (TID 8)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:22:28 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:22:28 ERROR Executor:91 - Exception in task 1.0 in stage 3.0 (TID 7)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:22:28 WARN  TaskSetManager:66 - Lost task 2.0 in stage 3.0 (TID 8, localhost, executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)

2019-07-01 19:22:28 ERROR TaskSetManager:70 - Task 2 in stage 3.0 failed 1 times; aborting job
2019-07-01 19:22:28 WARN  TaskSetManager:66 - Lost task 0.0 in stage 3.0 (TID 6, localhost, executor driver): TaskKilled (Stage cancelled)
2019-07-01 19:22:28 WARN  TaskSetManager:66 - Lost task 3.0 in stage 3.0 (TID 9, localhost, executor driver): TaskKilled (Stage cancelled)
2019-07-01 19:23:29 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:23:29 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:23:29 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:23:29 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)

2019-07-01 19:23:29 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-07-01 19:24:20 ERROR Executor:91 - Exception in task 1.0 in stage 1.0 (TID 2)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:24:20 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:24:20 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:24:20 ERROR Executor:91 - Exception in task 0.0 in stage 1.0 (TID 1)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-01 19:24:20 WARN  TaskSetManager:66 - Lost task 1.0 in stage 1.0 (TID 2, localhost, executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)

2019-07-01 19:24:20 ERROR TaskSetManager:70 - Task 1 in stage 1.0 failed 1 times; aborting job
2019-07-01 19:24:20 WARN  TaskSetManager:66 - Lost task 2.0 in stage 1.0 (TID 3, localhost, executor driver): TaskKilled (Stage cancelled)
2019-07-01 19:24:20 WARN  TaskSetManager:66 - Lost task 3.0 in stage 1.0 (TID 4, localhost, executor driver): TaskKilled (Stage cancelled)
2019-07-02 15:15:03 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:15:04 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:15:04 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:15:04 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:15:04 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:15:04 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:15:04 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:15:04 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.030580728s
2019-07-02 15:15:04 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:15:04 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:15:04 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.015127534s
2019-07-02 15:15:04 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:15:04 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-02 15:15:04 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:15:04 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:15:04 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-02 15:17:54 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:17:55 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:17:55 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:17:55 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:17:55 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:17:55 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:17:55 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:17:55 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.030780029s
2019-07-02 15:17:55 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:17:55 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:17:55 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.016767556s
2019-07-02 15:17:55 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:17:55 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-02 15:17:55 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:17:55 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:17:55 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-02 15:26:02 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:26:03 ERROR Executor:91 - Exception in task 2.0 in stage 0.0 (TID 2)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "/home/saba/Documents/Big Data Lab/numpymodel.py", line 90, in <lambda>
    train_data = record.map(lambda t: Sample.from_ndarray(t[0], t[1]))
  File "/usr/local/lib/python3.6/dist-packages/bigdl/util/common.py", line 334, in from_ndarray
    "features should be a list of np.ndarray, not %s" % type(features)
AssertionError: features should be a list of np.ndarray, not <class 'list'>

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-02 15:26:03 ERROR Executor:91 - Exception in task 7.0 in stage 0.0 (TID 7)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "/home/saba/Documents/Big Data Lab/numpymodel.py", line 90, in <lambda>
    train_data = record.map(lambda t: Sample.from_ndarray(t[0], t[1]))
  File "/usr/local/lib/python3.6/dist-packages/bigdl/util/common.py", line 334, in from_ndarray
    "features should be a list of np.ndarray, not %s" % type(features)
AssertionError: features should be a list of np.ndarray, not <class 'list'>

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-02 15:26:03 ERROR Executor:91 - Exception in task 5.0 in stage 0.0 (TID 5)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "/home/saba/Documents/Big Data Lab/numpymodel.py", line 90, in <lambda>
    train_data = record.map(lambda t: Sample.from_ndarray(t[0], t[1]))
  File "/usr/local/lib/python3.6/dist-packages/bigdl/util/common.py", line 334, in from_ndarray
    "features should be a list of np.ndarray, not %s" % type(features)
AssertionError: features should be a list of np.ndarray, not <class 'list'>

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-02 15:26:03 WARN  TaskSetManager:66 - Lost task 7.0 in stage 0.0 (TID 7, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "/home/saba/Documents/Big Data Lab/numpymodel.py", line 90, in <lambda>
    train_data = record.map(lambda t: Sample.from_ndarray(t[0], t[1]))
  File "/usr/local/lib/python3.6/dist-packages/bigdl/util/common.py", line 334, in from_ndarray
    "features should be a list of np.ndarray, not %s" % type(features)
AssertionError: features should be a list of np.ndarray, not <class 'list'>

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:26:03 ERROR TaskSetManager:70 - Task 7 in stage 0.0 failed 1 times; aborting job
2019-07-02 15:27:31 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:27:32 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:27:32 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:27:32 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:27:32 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:27:32 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:27:32 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:27:32 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.030561182s
2019-07-02 15:27:32 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:27:32 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:27:32 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.014645446s
2019-07-02 15:27:32 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:27:32 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-02 15:27:32 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:27:32 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:27:32 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-02 15:29:32 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:29:33 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:29:33 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:29:33 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:29:33 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:29:33 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:29:33 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:29:33 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.034894816s
2019-07-02 15:29:33 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:29:33 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:29:33 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.012754093s
2019-07-02 15:29:33 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:29:33 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-02 15:29:33 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:29:33 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:29:33 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-02 15:30:11 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:30:12 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:30:12 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:30:12 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:30:12 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:30:12 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:30:12 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:30:12 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.032469422s
2019-07-02 15:30:12 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:30:12 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:30:12 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018020276s
2019-07-02 15:30:12 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 15:30:12 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:30:12 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:30:12 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:30:12 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:30:12 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 15:30:12 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:30:12 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:30:12 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:36:23 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:36:24 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:36:24 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:36:24 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:36:24 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:36:24 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:36:24 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:36:24 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.029968305s
2019-07-02 15:36:24 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:36:24 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:36:24 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.016648305s
2019-07-02 15:36:24 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 15:36:24 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:36:24 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:36:24 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:36:24 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:36:24 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 15:36:24 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:36:24 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:36:24 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:44:50 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:44:51 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:44:51 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:44:51 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:44:51 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:44:51 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:44:51 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:44:51 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.030980934s
2019-07-02 15:44:51 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:44:51 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:44:51 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.012729672s
2019-07-02 15:44:51 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 15:44:51 ERROR ThreadPool$:147 - Error: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:44:51 ERROR ThreadPool$:147 - Error: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:44:51 ERROR ThreadPool$:147 - Error: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:44:51 ERROR ThreadPool$:147 - Error: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:44:51 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 15:44:51 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:44:51 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:44:51 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Sequential[3ab0fa22]{
  [input -> (1) -> (2) -> output]
  (1): LookupTable[bdcb4d26](nIndex=8,nOutput=2,paddingValue=0.0,normType=2.0)
  (2): Reshape[4da972ed](1x4)
}/Reshape[4da972ed](1x4)
java.lang.IllegalArgumentException: requirement failed: element number must match Reshape size. But In Reshape4da972ed : element number is: 12 , reshape size is: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:75)
	at com.intel.analytics.bigdl.nn.Reshape.updateOutput(Reshape.scala:46)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:46:15 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:46:16 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:46:16 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:46:16 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:46:16 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:46:16 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:46:16 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:46:16 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.039666877s
2019-07-02 15:46:16 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:46:16 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:46:16 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.019323175s
2019-07-02 15:46:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 15:46:16 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:46:16 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:46:16 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:46:16 ERROR ThreadPool$:147 - Error: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:46:16 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 15:46:16 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:46:16 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:46:16 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.IllegalArgumentException: requirement failed: ClassNLLCriterion: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    input dim(3)
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:90)
	at com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:69)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:46:56 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:46:56 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:46:56 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:46:56 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:46:56 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:46:56 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:46:56 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:46:56 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.056585818s
2019-07-02 15:46:56 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:46:56 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:46:56 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.013133954s
2019-07-02 15:46:57 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 15:46:57 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:46:57 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:46:57 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:46:57 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:46:57 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 15:46:57 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:46:57 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:46:57 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:47:44 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:47:45 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:47:45 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:47:45 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:47:45 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:47:45 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:47:45 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:47:45 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.032568195s
2019-07-02 15:47:45 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:47:45 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:47:45 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018953356s
2019-07-02 15:47:45 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 15:47:45 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:47:45 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:47:45 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:47:45 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:47:45 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 15:47:45 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:47:45 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:47:45 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:51:35 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:51:36 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:51:36 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:51:36 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:51:36 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:51:36 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:51:36 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:51:36 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.028692399s
2019-07-02 15:51:36 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:51:36 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:51:36 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.015973561s
2019-07-02 15:51:36 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 15:51:36 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:51:36 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:51:36 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:51:36 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:51:36 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 15:51:36 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:51:36 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:51:36 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:53:05 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 15:53:06 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 15:53:06 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 15:53:06 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 15:53:06 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 15:53:06 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 15:53:06 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 15:53:06 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.029073282s
2019-07-02 15:53:06 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 15:53:06 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 15:53:06 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.016810387s
2019-07-02 15:53:06 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 15:53:06 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:53:06 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:53:06 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:53:06 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 15:53:06 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 15:53:06 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 15:53:06 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 15:53:06 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 16:00:53 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 16:00:54 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 16:00:54 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 16:00:54 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 16:00:54 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 16:00:54 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 16:00:54 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 16:00:54 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.031510101s
2019-07-02 16:00:54 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 16:00:54 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 16:00:54 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.015681419s
2019-07-02 16:00:54 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 16:00:54 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 16:00:54 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 16:00:54 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 16:00:54 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 16:00:54 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 16:00:54 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 16:00:54 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 16:00:54 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 16:06:15 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 16:06:16 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 16:06:16 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 16:06:16 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 16:06:16 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 16:06:16 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 16:06:16 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 16:06:16 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.036041519s
2019-07-02 16:06:16 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 16:06:16 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 16:06:16 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.021278578s
2019-07-02 16:06:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 16:06:16 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 16:06:16 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 16:06:16 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 16:06:16 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 16:06:16 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 16:06:16 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 16:06:16 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 16:06:16 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.tensor.DenseTensor cannot be cast to com.intel.analytics.bigdl.utils.Table
	at com.intel.analytics.bigdl.nn.MarginRankingCriterion.updateOutput(MarginRankingCriterion.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 16:21:42 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 16:21:43 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 16:21:43 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 16:21:43 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 16:21:43 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 16:21:43 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 16:21:43 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 16:21:43 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.034839227s
2019-07-02 16:21:43 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 16:21:43 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 16:21:43 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.021020228s
2019-07-02 16:21:43 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 16:21:43 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.148848643s] Trained 4 records in 0.148848643 seconds. Throughput is 26.872936 records/second. Loss is 0.97128296. Sequential88a7ec9a's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 16:21:43 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.148848643s] Epoch finished. Wall clock time is 162.866848 ms
2019-07-02 16:41:13 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 16:41:14 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 16:41:14 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 16:41:14 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 16:41:14 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 16:41:14 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 16:41:14 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 16:41:14 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.029964887s
2019-07-02 16:41:14 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 16:41:14 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 16:41:14 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.013708448s
2019-07-02 16:41:14 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 16:41:14 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.13337755s] Trained 4 records in 0.13337755 seconds. Throughput is 29.990053 records/second. Loss is 0.8574829. Sequential353145e6's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 16:41:14 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.13337755s] Epoch finished. Wall clock time is 143.536244 ms
2019-07-02 16:43:00 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 16:43:01 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 16:43:01 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 16:43:01 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 16:43:01 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 16:43:01 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 16:43:01 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 16:43:01 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.033639256s
2019-07-02 16:43:01 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 16:43:01 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 16:43:01 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.014011285s
2019-07-02 16:43:01 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 16:43:01 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.109845311s] Trained 4 records in 0.109845311 seconds. Throughput is 36.414845 records/second. Loss is 1.1602274. Sequential62b00b36's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 16:43:01 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.109845311s] Epoch finished. Wall clock time is 119.122778 ms
2019-07-02 17:06:22 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:06:23 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:06:23 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:06:23 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:06:23 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:06:23 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:06:23 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:06:23 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.038241775s
2019-07-02 17:06:23 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:06:23 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:06:23 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.01889452s
2019-07-02 17:06:23 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:06:23 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.12454232s] Trained 4 records in 0.12454232 seconds. Throughput is 32.117596 records/second. Loss is 1.2159323. Sequential50866bc8's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 17:06:23 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.12454232s] Epoch finished. Wall clock time is 135.513685 ms
2019-07-02 17:06:37 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:06:38 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:06:38 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:06:38 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:06:38 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:06:38 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:06:38 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:06:38 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.039833938s
2019-07-02 17:06:38 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:06:38 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:06:38 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.016220081s
2019-07-02 17:06:38 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:06:38 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.136956592s] Trained 4 records in 0.136956592 seconds. Throughput is 29.206335 records/second. Loss is 0.8215129. Sequential59330055's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 17:06:38 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.136956592s] Epoch finished. Wall clock time is 146.867764 ms
2019-07-02 17:06:55 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:06:55 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:06:55 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:06:55 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:06:55 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:06:55 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:06:55 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:06:55 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.038999417s
2019-07-02 17:06:55 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:06:55 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:06:55 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018370996s
2019-07-02 17:06:55 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:06:55 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.142066645s] Trained 4 records in 0.142066645 seconds. Throughput is 28.1558 records/second. Loss is 1.2185326. Sequential482dc21c's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 17:06:55 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.142066645s] Epoch finished. Wall clock time is 152.929289 ms
2019-07-02 17:07:15 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:07:16 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:07:16 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:07:16 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:07:16 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:07:16 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:07:16 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:07:16 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.041626446s
2019-07-02 17:07:16 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:07:16 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:07:16 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.016713315s
2019-07-02 17:07:16 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:07:17 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.151053013s] Trained 4 records in 0.151053013 seconds. Throughput is 26.48077 records/second. Loss is 1.0341313. Sequential71ccbfd4's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 17:07:17 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.151053013s] Epoch finished. Wall clock time is 161.941742 ms
2019-07-02 17:07:23 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:07:23 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:07:23 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:07:23 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:07:23 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:07:23 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:07:23 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:07:24 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.040728559s
2019-07-02 17:07:24 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:07:24 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:07:24 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.019007373s
2019-07-02 17:07:24 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:07:24 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.129541332s] Trained 4 records in 0.129541332 seconds. Throughput is 30.878174 records/second. Loss is 1.1676432. Sequential4c5055f1's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 17:07:24 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.129541332s] Epoch finished. Wall clock time is 138.889132 ms
2019-07-02 17:07:42 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:07:42 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:07:42 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:07:42 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:07:42 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:07:42 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:07:42 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:07:42 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.031186187s
2019-07-02 17:07:42 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:07:42 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:07:42 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.019036223s
2019-07-02 17:07:42 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:07:42 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.13400409s] Trained 4 records in 0.13400409 seconds. Throughput is 29.849836 records/second. Loss is 1.4041647. Sequential3b317390's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 17:07:42 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.13400409s] Epoch finished. Wall clock time is 147.221816 ms
2019-07-02 17:08:13 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:08:13 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:08:13 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:08:13 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:08:13 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:08:13 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:08:13 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:08:13 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.034086406s
2019-07-02 17:08:13 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:08:13 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:08:13 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.02186535s
2019-07-02 17:08:13 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:08:14 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.144029918s] Trained 4 records in 0.144029918 seconds. Throughput is 27.772009 records/second. Loss is 0.97076416. Sequential9bb8d88c's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 17:08:14 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.144029918s] Epoch finished. Wall clock time is 154.231096 ms
2019-07-02 17:09:41 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:09:42 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:09:42 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:09:42 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:09:42 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:09:42 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:09:42 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:09:42 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.037615375s
2019-07-02 17:09:42 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:09:42 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:09:42 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017948833s
2019-07-02 17:09:42 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:09:42 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.1358917s] Trained 4 records in 0.1358917 seconds. Throughput is 29.435204 records/second. Loss is 1.3863525. Sequentialff93c2d5's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 17:09:42 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.1358917s] Epoch finished. Wall clock time is 145.778639 ms
2019-07-02 17:10:19 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:10:19 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:10:19 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:10:19 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:10:19 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:10:19 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:10:19 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:10:19 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.033181216s
2019-07-02 17:10:19 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:10:19 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:10:19 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017099402s
2019-07-02 17:10:19 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:10:19 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 17:10:19 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 17:10:19 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 17:10:19 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 17:10:19 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 17:10:20 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 17:10:20 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 17:10:20 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 17:12:37 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:12:38 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:12:38 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:12:38 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:12:38 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:12:38 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:12:38 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:12:38 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.035779176s
2019-07-02 17:12:38 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:12:38 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:12:38 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.021042783s
2019-07-02 17:12:38 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:12:38 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 17:12:38 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 17:12:38 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 17:12:38 ERROR ThreadPool$:147 - Error: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 17:12:38 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 17:12:38 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 17:12:38 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 17:12:38 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.lang.ClassCastException: com.intel.analytics.bigdl.utils.Table cannot be cast to com.intel.analytics.bigdl.tensor.Tensor
	at com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:265)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 17:22:39 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:22:40 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:22:40 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:22:40 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:22:40 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:22:40 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:22:40 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:22:40 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.036948925s
2019-07-02 17:22:40 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:22:40 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:22:40 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017931254s
2019-07-02 17:22:40 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:22:40 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.125122159s] Trained 4 records in 0.125122159 seconds. Throughput is 31.968758 records/second. Loss is 1.3154501. Sequential13115845's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 17:22:40 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.125122159s] Epoch finished. Wall clock time is 134.402993 ms
2019-07-02 17:23:33 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:23:34 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:23:34 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:23:34 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:23:34 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:23:34 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:23:34 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:23:34 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.038908958s
2019-07-02 17:23:34 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:23:34 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:23:34 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.018504331s
2019-07-02 17:23:34 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:23:34 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.138099594s] Trained 4 records in 0.138099594 seconds. Throughput is 28.964603 records/second. Loss is 0.973999. Sequentiald57a804b's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 17:23:34 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.138099594s] Epoch finished. Wall clock time is 148.982329 ms
2019-07-02 17:25:45 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 17:25:46 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 17:25:46 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 17:25:46 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 17:25:46 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 17:25:46 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 17:25:46 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 17:25:46 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.036249653s
2019-07-02 17:25:46 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 17:25:46 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 17:25:46 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.019865247s
2019-07-02 17:25:46 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 17:25:46 INFO  DistriOptimizer$:408 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.162097818s] Trained 4 records in 0.162097818 seconds. Throughput is 24.676458 records/second. Loss is 1.2620544. Sequential67fbcfc3's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-07-02 17:25:46 INFO  DistriOptimizer$:452 - [Epoch 1 4/4][Iteration 1][Wall Clock 0.162097818s] Epoch finished. Wall clock time is 173.281427 ms
2019-07-02 20:53:53 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 20:53:54 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR Executor:91 - Exception in task 2.0 in stage 0.0 (TID 2)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR Executor:91 - Exception in task 5.0 in stage 0.0 (TID 5)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR Executor:91 - Exception in task 7.0 in stage 0.0 (TID 7)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR Executor:91 - Exception in task 4.0 in stage 0.0 (TID 4)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR Executor:91 - Exception in task 3.0 in stage 0.0 (TID 3)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR Executor:91 - Exception in task 6.0 in stage 0.0 (TID 6)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 ERROR Executor:91 - Exception in task 1.0 in stage 0.0 (TID 1)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:53:54 WARN  TaskSetManager:66 - Lost task 7.0 in stage 0.0 (TID 7, localhost, executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)

2019-07-02 20:53:54 ERROR TaskSetManager:70 - Task 7 in stage 0.0 failed 1 times; aborting job
2019-07-02 20:56:01 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 20:56:02 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 20:56:02 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 20:56:02 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 20:56:02 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 20:56:02 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 20:56:02 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 20:56:02 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.061232571s
2019-07-02 20:56:02 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 20:56:02 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 20:56:02 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.011791636s
2019-07-02 20:56:02 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 20:56:02 ERROR ThreadPool$:147 - Error: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 20:56:02 ERROR ThreadPool$:147 - Error: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 20:56:02 ERROR ThreadPool$:147 - Error: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 20:56:02 ERROR ThreadPool$:147 - Error: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 20:56:02 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 20:56:02 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 20:56:02 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 20:56:02 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Sequential[8d65b37d]{
  [input -> (1) -> output]
  (1): LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[3d54e02d](nIndex=96,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 20:57:18 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 20:57:19 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 ERROR PythonRunner:91 - Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 362, in main
    eval_type = read_int(infile)
  File "/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 717, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 ERROR PythonRunner:91 - This may have been caused by a prior exception:
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 ERROR Executor:91 - Exception in task 6.0 in stage 0.0 (TID 6)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 ERROR Executor:91 - Exception in task 2.0 in stage 0.0 (TID 2)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 ERROR Executor:91 - Exception in task 4.0 in stage 0.0 (TID 4)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
2019-07-02 20:57:19 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.hasNext(RDD.scala:869)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$26$$anon$2.foreach(RDD.scala:865)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)

2019-07-02 20:57:19 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-07-02 20:57:19 WARN  TaskSetManager:66 - Lost task 5.0 in stage 0.0 (TID 5, localhost, executor driver): TaskKilled (Stage cancelled)
2019-07-02 20:57:19 WARN  TaskSetManager:66 - Lost task 3.0 in stage 0.0 (TID 3, localhost, executor driver): TaskKilled (Stage cancelled)
2019-07-02 20:57:19 WARN  TaskSetManager:66 - Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): TaskKilled (Stage cancelled)
2019-07-02 20:57:19 WARN  TaskSetManager:66 - Lost task 7.0 in stage 0.0 (TID 7, localhost, executor driver): TaskKilled (Stage cancelled)
2019-07-02 20:58:21 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 20:58:22 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 20:58:22 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 20:58:22 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 20:58:22 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 20:58:22 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 20:58:22 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 20:58:22 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.059284318s
2019-07-02 20:58:22 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 20:58:22 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 20:58:22 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.016728046s
2019-07-02 20:58:22 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 20:58:22 ERROR ThreadPool$:147 - Error: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 20:58:22 ERROR ThreadPool$:147 - Error: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 20:58:22 ERROR ThreadPool$:147 - Error: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 20:58:22 ERROR ThreadPool$:147 - Error: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 20:58:22 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 20:58:22 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 20:58:22 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 20:58:22 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Sequential[43353906]{
  [input -> (1) -> output]
  (1): LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[48793735](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 21:01:10 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 21:01:11 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 21:01:11 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 21:01:11 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 21:01:11 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 21:01:11 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 21:01:11 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 21:01:11 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.060064409s
2019-07-02 21:01:11 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 21:01:11 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 21:01:11 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.015709048s
2019-07-02 21:01:11 ERROR ThreadPool$:197 - Error: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 21:01:11 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-07-02 21:01:11 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 21:01:11 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 21:01:11 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: requirement failed: total batch size: 1 should be divided by total core number: 4
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply$mcV$sp(DistriOptimizer.scala:226)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$apply$1.apply(DistriOptimizer.scala:225)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:194)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-07-02 21:01:44 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 21:01:45 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 21:01:45 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 21:01:45 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 21:01:45 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 21:01:45 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 21:01:45 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 21:01:45 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.034802049s
2019-07-02 21:01:45 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 21:01:45 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 21:01:45 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.017091056s
2019-07-02 21:01:45 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 21:01:45 ERROR ThreadPool$:147 - Error: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 21:01:45 ERROR ThreadPool$:147 - Error: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 21:01:45 ERROR ThreadPool$:147 - Error: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 21:01:45 ERROR ThreadPool$:147 - Error: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 21:01:45 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 12)
java.util.concurrent.ExecutionException: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 21:01:45 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 21:01:45 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-07-02 21:01:45 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 12, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Sequential[a17a35e6]{
  [input -> (1) -> output]
  (1): LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[65b5a7a](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 21:02:57 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-07-02 21:02:57 INFO  DistriOptimizer$:624 - Cache thread models...
2019-07-02 21:02:57 INFO  DistriOptimizer$:606 - model thread pool size is 1
2019-07-02 21:02:57 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-07-02 21:02:57 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-07-02 21:02:57 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-07-02 21:02:57 INFO  DistriOptimizer$:154 - Count dataset
2019-07-02 21:02:57 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.03285436s
2019-07-02 21:02:57 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-07-02 21:02:57 INFO  DistriOptimizer$:170 - Shuffle data
2019-07-02 21:02:57 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.013252019s
2019-07-02 21:02:57 WARN  DistriOptimizer$:230 - Warning: for better training speed, total batch size is recommended to be at least two times of core number4, please tune your batch size accordingly
2019-07-02 21:02:57 ERROR ThreadPool$:147 - Error: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 21:02:57 ERROR ThreadPool$:147 - Error: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 21:02:57 ERROR ThreadPool$:147 - Error: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 21:02:57 ERROR ThreadPool$:147 - Error: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-07-02 21:02:57 ERROR Executor:91 - Exception in task 0.0 in stage 11.0 (TID 15)
java.util.concurrent.ExecutionException: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
2019-07-02 21:02:58 WARN  TaskSetManager:66 - Lost task 0.0 in stage 11.0 (TID 15, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

2019-07-02 21:02:58 ERROR TaskSetManager:70 - Task 0 in stage 11.0 failed 1 times; aborting job
2019-07-02 21:02:58 ERROR DistriOptimizer$:894 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 15, localhost, executor driver): java.util.concurrent.ExecutionException: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:342)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$8.apply(DistriOptimizer.scala:284)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:212)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: Layer info: Sequential[b951777e]{
  [input -> (1) -> output]
  (1): LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
}/LookupTable[a276bf1d](nIndex=49,nOutput=2,paddingValue=0.0,normType=2.0)
java.lang.IllegalArgumentException: requirement failed: LookupTable: 
 The input to the layer needs to be a vector(or a mini-batch of vectors);
 please use the Reshape module to convert multi-dimensional input into vectors
 if appropriate"
    , input dim [4]
	at scala.Predef$.require(Predef.scala:224)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:174)
	at com.intel.analytics.bigdl.nn.LookupTable.updateOutput(LookupTable.scala:47)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Sequential.updateOutput(Sequential.scala:39)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:264)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$6$$anonfun$apply$2.apply(DistriOptimizer.scala:255)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:144)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

